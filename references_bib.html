<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>

<head>
<title>master_thesis.bib</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta name="generator" content="bibtex2html">
</head>

<body>
<h1>master_thesis.bib</h1><a name="Farabet2014"></a><pre>
@phdthesis{<a href="references.html#Farabet2014">Farabet2014</a>,
  abstract = {One of the open questions of artificial computer vision is how to produce good internal representations of the visual world. What sort of internal representation would allow an artificial vision system to detect and classify objects into categories, independently of pose, scale, illumination, conforma- tion, and clutter? More interestingly, how could an artificial vision system learn appropriate internal representations automatically, the way animals and humans seem to learn by simply looking at the world? Another related question is that of computational tractability, and more precisely that of computational efficiency. Given a good visual represen- tation, how efficiently can it be trained, and used to encode new sensorial data. Efficiency has several dimensions: power requirements, processing speed, and memory usage. In this thesis I present three new contributions to the field of computer vision: (1) a multiscale deep convolutional network architecture to easily capture long-distance relationships between input variables in image data, (2) a tree-based algorithm to efficiently explore multiple segmentation can- didates, to produce maximally confident semantic segmentations of images, (3) a custom dataflow computer architecture optimized for the computation of convolutional networks, and similarly dense image processing models. All three contributions were produced with the common goal of getting us closer to real-time image understanding. Scene parsing consists in labeling each pixel in an image with the category of the object it belongs to. In the first part of this thesis, I propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features. Inparallel to feature extraction, a tree of segments is computed from a graph of pixel dissimilarities. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average “purity” of the class distributions, hence maximizing the overall likelihood that each segment contains a single object. The system yields record accuracies on several public benchmarks. The computation of convolutional networks, and related models heavily relies on a set of basic operators that are particularly fit for dedicated hardware implementations. In the second part of this thesis I introduce a scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms—neuFlow —and a dataflow compiler— luaFlow —that transforms high-level flow-graph representations of these al- gorithms into machine code for neuFlow. This system was designed with the goal of providing real-time detection, categorization and localization of objects in complex scenes, while consuming 10 Watts when implemented on a Xilinx Virtex 6 FPGA platform, or about ten times less than a lap- top computer, and producing speedups of up to 100 times in real-world applications (results from 2011).},
  author = {Farabet, Cl\'{e}ment},
  file = {:home/maikel/Documents/mendeley/thA\_se.pdf:pdf},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  pages = {124},
  school = {Universit\'{e} Paris-Est},
  title = {{Towards Real-Time Image Understanding with Convolutional Networks}},
  url = {<a href="http://hal-upec-upem.archives-ouvertes.fr/docs/00/96/56/22/PDF/thA\_se.pdf">http://hal-upec-upem.archives-ouvertes.fr/docs/00/96/56/22/PDF/thA\_se.pdf</a>},
  year = {2014}
}
</pre>

<a name="Marszalek2009"></a><pre>
@article{<a href="references.html#Marszalek2009">Marszalek2009</a>,
  abstract = {This paper exploits the context of natural dynamic scenes for human action recognition in video. Human actions are frequently constrained by the purpose and the physical properties of scenes and demonstrate high correlation with particular scene classes. For example, eating often happens in a kitchen while running is more common outdoors. The contribution of this paper is three-fold: (a) we automatically discover relevant scene classes and their correlation with human actions, (b) we show how to learn selected scene classes from video without manual supervision and (c) we develop a joint framework for action and scene recognition and demonstrate improved recognition of both in natural video. We use movie scripts as a means of automatic supervision for training. For selected action classes we identify correlated scene classes in text and then retrieve video samples of actions and scenes for training using script-to-video alignment. Our visual models for scenes and actions are formulated within the bag-of-features framework and are combined in a joint scene-action SVM-based classifier. We report experimental results and validate the method on a new large dataset with twelve action classes and ten scene classes acquired from 69 movies.},
  author = {Marszalek, M and Laptev, Ivan and Schmid, Cordelia},
  file = {:home/maikel/Documents/mendeley/05206557.pdf:pdf},
  isbn = {9781424439911},
  journal = {Computer Vision and \ldots},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  number = {i},
  pages = {2929--2936},
  title = {{Actions in context}},
  url = {<a href="http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5206557">http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5206557</a>},
  year = {2009}
}
</pre>

<a name="Le2010"></a><pre>
@article{<a href="references.html#Le2010">Le2010</a>,
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hardcoding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled ” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs’ advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.},
  author = {Le, QV and Ngiam, Jiquan and Chen, Zhenghao and hao Chia, DJ and Koh, PW},
  file = {:home/maikel/Documents/mendeley/4136-tiled-convolutional-neural-networks.pdf:pdf},
  journal = {NIPS},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  pages = {1--9},
  title = {{Tiled convolutional neural networks.}},
  url = {https://papers.nips.cc/paper/4136-tiled-convolutional-neural-networks.pdf},
  year = {2010}
}
</pre>

<a name="Le2011a"></a><pre>
@article{<a href="references.html#Le2011a">Le2011a</a>,
  abstract = {Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3\% and 75.8\% respectively, which are approximately 5\% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/\~{}wzou/.},
  author = {Le, Quoc V. and Zou, Will Y. and Yeung, Serena Y. and Ng, Andrew Y.},
  doi = {10.1109/CVPR.2011.5995496},
  file = {:home/maikel/Documents/mendeley/cvpr\_LeZouYeungNg11.pdf:pdf},
  isbn = {978-1-4577-0394-2},
  journal = {Cvpr 2011},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  month = jun,
  pages = {3361--3368},
  publisher = {Ieee},
  title = {{Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis}},
  url = {<a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995496">http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995496</a>},
  year = {2011}
}
</pre>

<a name="Hinton2012"></a><pre>
@article{<a href="references.html#Hinton2012">Hinton2012</a>,
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item Paper about Dropout
  $\backslash$item Standard way to reduce test error
    $\backslash$begin\{itemize\}
      $\backslash$item averaging different models
      $\backslash$item Computationally expensive in training and test
    $\backslash$end\{itemize\}
  $\backslash$item Dropout
    $\backslash$begin\{itemize\}
      $\backslash$item Small training set
      $\backslash$item Prevents ``overfitting''
      $\backslash$item They use \$50\backslash\%\$
      $\backslash$item Instead of L2 norm, they set an upper bound for each individual
        neuron.
      $\backslash$item Mean network : At test time divide all the outgoing weights
        by 2 to compensate dropout
      $\backslash$item Specific case
        $\backslash$begin\{itemize\}
          $\backslash$item Single hidden layer network
          $\backslash$item N hidden units
          $\backslash$item ``Softmax'' output
          $\backslash$item \$50\backslash\%\$ dropout
          $\backslash$item during test using mean network
          $\backslash$item Exactly equivalent to taking the geometric mean of the
            probability distributions over labels predicted by all \$2\^{}N\$
            possible networks
        $\backslash$end\{itemize\}
    $\backslash$end\{itemize\}
  $\backslash$item Results
    $\backslash$begin\{itemize\}
      $\backslash$item MNIST
        $\backslash$begin\{itemize\}
          $\backslash$item No dropout : 160 errors
          $\backslash$item Dropbout : 130 errors
          $\backslash$item Dropout + rm random \$20\backslash\%\$ pixels : 110 errors
          $\backslash$item Deep Boltzmann machine : 88 errors
          $\backslash$item + Dropout : 79 errors
            $\backslash$figuremacro\{figures/Hinton2012\_fig5\}\{Visualization of features learned by first layer hidden units\}\{left without dropout and right using dropout\}
        $\backslash$end\{itemize\}
      $\backslash$item TIMIT
        $\backslash$begin\{itemize\}
          $\backslash$item 4 Fully-connected hidden layers 4000 units per layer
          $\backslash$item + 185 ``softmax'' output units
          $\backslash$item Without dropout : \$22.7\backslash\%\$
          $\backslash$item Dropout on hidden units : \$19.7\backslash\%\$
        $\backslash$end\{itemize\}
      $\backslash$item CIFAR-10
        $\backslash$begin\{itemize\}
          $\backslash$item Best published : \$18.5\backslash\%\$
          $\backslash$item 3 Conv+Max-pool 1 Fully : \$16.6\backslash\%\$
          $\backslash$item + Dropout in last hidden layer : \$15.6\backslash\%\$
        $\backslash$end\{itemize\}
      $\backslash$item ImageNet
        $\backslash$begin\{itemize\}
          $\backslash$item Average of 6 separate models : \$47.2\backslash\%\$
          $\backslash$item state-of-the-art : \$45.7\backslash\%\$
          $\backslash$item 5 Conv+Max-pool
          $\backslash$item + 2 Fully
          $\backslash$item + 1000 ``softmax''
          $\backslash$item Without dropout : \$48.6\backslash\%\$
          $\backslash$item Dropout in the 6th : \$42.4\backslash\%\$
        $\backslash$end\{itemize\}
      $\backslash$item Reuters
        $\backslash$begin\{itemize\}
          $\backslash$item 2 fully of 2000 hidden units
          $\backslash$item Without dropout : \$31.05\backslash\%\$
          $\backslash$item Dropout : \$29.62\backslash\%\$
        $\backslash$end\{itemize\}
    $\backslash$end\{itemize\}
$\backslash$end\{itemize\}},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1207.0580v1},
  author = {Hinton, GE and Srivastava, N and Krizhevsky, Alex and Sutskever, I and Salakhutdinov, RR},
  eprint = {arXiv:1207.0580v1},
  file = {:home/maikel/Documents/mendeley/1207.0580v1.pdf:pdf},
  journal = {arXiv preprint arXiv: \ldots},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  pages = {1--18},
  title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
  url = {<a href="http://arxiv.org/abs/1207.0580 http://arxiv.org/pdf/1207.0580v1.pdf">http://arxiv.org/abs/1207.0580 http://arxiv.org/pdf/1207.0580v1.pdf</a>},
  year = {2012}
}
</pre>

<a name="Fan2014"></a><pre>
@article{<a href="references.html#Fan2014">Fan2014</a>,
  abstract = {Face representation is a crucial step of face recognition systems. An optimal face representation should be discriminative, robust, compact, and very easy-to-implement. While numerous hand-crafted and learning-based representations have been proposed, considerable room for improvement is still present. In this paper, we present a very easy-to-implement deep learning framework for face representation. Our method bases on a new structure of deep network (called Pyramid CNN). The proposed Pyramid CNN adopts a greedy-filter-and-down-sample operation, which enables the training procedure to be very fast and computation-efficient. In addition, the structure of Pyramid CNN can naturally incorporate feature sharing across multi-scale face representations, increasing the discriminative ability of resulting representation. Our basic network is capable of achieving high recognition accuracy (85.8\% on LFW benchmark) with only 8 dimension representation. When extended to feature-sharing Pyramid CNN, our system achieves the state-of-the-art performance (97.3\%) on LFW benchmark. We also introduce a new benchmark of realistic face images on social network and validate our proposed representation has a good ability of generalization.},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item New deep structure Pyramid CNN
  $\backslash$item Labeled Faces in the Wild (LFW)
    $\backslash$begin\{itemize\}
      $\backslash$item \$> 13.000\$ faces
      $\backslash$item 1680 of the people have two or more distinct photos
      $\backslash$item Detected by Viola-Jones detector
      $\backslash$item http://vis-www.cs.umass.edu/lfw/
    $\backslash$end\{itemize\}
  $\backslash$item State-of-the-art performance on LFW benchmark (\$97.3\backslash\%\$)
        
  $\backslash$item Good face representation
    $\backslash$begin\{itemize\}
      $\backslash$item Identity-preserving: Same person pictures close in feature space
      $\backslash$item Abstract and Compact: from high to low dimensionality
      $\backslash$item Uniform and Automatic: NO hand-crafted and hard-wired parts
    $\backslash$end\{itemize\}
        
  $\backslash$item Pyramid CNN
    $\backslash$begin\{itemize\}
      $\backslash$item ID-preserving Representation Learning: Loss functions measures
        distance in output feature space
      $\backslash$item Convolutions and Down-sampling
      $\backslash$item Deeper give best results, but increases rapidly the training time
      $\backslash$item Each CNN own private output layer and gets the input from the previous
        shared layer
      $\backslash$item Only the output of the last level network is used for the represetnation
      $\backslash$item The rest of the outputs is just for training
    $\backslash$end\{itemize\}
  $\backslash$item Results
    $\backslash$begin\{itemize\}
      $\backslash$item 164 incorrect predictions
      $\backslash$item Some of them are incorrectly labeled
      $\backslash$item Others are very difficult for humans, because of the age or pose
      $\backslash$item On LFW benchmark achieves state-of-the-art and close to human on croped images
    $\backslash$end\{itemize\}
  $\backslash$item With ROC curve as a mesure there is an improvement of 0.07-0.12 with Baseline
  $\backslash$item Face recognition does not contemplate affine transformations or perspectives,
  $\backslash$item Can be difficult to apply in task such as ImageNet, where the object can be
    in any place and position
$\backslash$end\{itemize\}},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1403.2802v1},
  author = {Fan, Haoqiang and Cao, Zhimin and Jiang, Yunin and Yin, Qi and Doudou, C and Doudou, Chinchilla},
  eprint = {arXiv:1403.2802v1},
  file = {:home/maikel/Documents/mendeley/1403.2802v1.pdf:pdf},
  journal = {arXiv preprint arXiv:1403.2802},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  pages = {1--10},
  title = {{Learning Deep Face Representation}},
  url = {<a href="http://arxiv.org/abs/1403.2802 http://arxiv.org/pdf/1403.2802v1.pdf">http://arxiv.org/abs/1403.2802 http://arxiv.org/pdf/1403.2802v1.pdf</a>},
  year = {2014}
}
</pre>

<a name="Hao2012"></a><pre>
@article{<a href="references.html#Hao2012">Hao2012</a>,
  abstract = {In this paper, we consider the problem of modeling complex texture information using undirected probabilistic graphical models. Texture is a special type of data that one can better understand by considering its local structure. For that purpose, we propose a convolutional variant of the Gaussian gated Boltzmann machine (GGBM) [12], inspired by the co-occurrence matrix in traditional texture analysis. We also link the proposed model to a much simpler Gaussian restricted Boltzmann machine where convolutional features are computed as a preprocessing step. The usefulness of the model is illustrated in texture classification and reconstruction experiments.},
  author = {Hao, Tele and Raiko, Tapani and Ilin, Alexander and Karhunen, Juha},
  file = {:home/maikel/Documents/mendeley/icann12hao (2).pdf:pdf},
  journal = {\ldots Neural Networks and Machine \ldots},
  keywords = {color,deep learn-,gated boltzmann machine,gaussian restricted boltzmann machine,ing,mscthesis,texture analysis},
  mendeley-tags = {color,mscthesis},
  title = {{Gated boltzmann machine in texture modeling}},
  url = {<a href="http://link.springer.com/chapter/10.1007/978-3-642-33266-1\_16 http://research.ics.aalto.fi/publications/bibdb2012/public\_pdfs/icann12hao.pdf">http://link.springer.com/chapter/10.1007/978-3-642-33266-1\_16 http://research.ics.aalto.fi/publications/bibdb2012/public\_pdfs/icann12hao.pdf</a>},
  year = {2012}
}
</pre>

<a name="Hoyer2000"></a><pre>
@article{<a href="references.html#Hoyer2000">Hoyer2000</a>,
  abstract = {Previous work has shown that independent component analysis (ICA) applied to feature extraction from natural image data yields features resembling Gabor functions and simple-cell receptive fields. This article considers the effects of including chromatic and stereo information. The inclusion of colour leads to features divided into separate red/green, blue/yellow, and bright/dark channels. Stereo image data, on the other hand, leads to binocular receptive fields which are tuned to various disparities. The similarities between these results and the observed properties of simple cells in the primary visual cortex are further evidence for the hypothesis that visual cortical neurons perform some type of redundancy reduction, which was one of the original motivations for ICA in the first place. In addition, ICA provides a principled method for feature extraction from colour and stereo images; such features could be used in image processing operations such as denoising and compression, as well as in pattern recognition.},
  author = {Hoyer, P O and Hyv\"{a}rinen, a},
  file = {:home/maikel/Documents/mendeley/Network00 (1).pdf:pdf},
  isbn = {3589451327},
  issn = {0954-898X},
  journal = {Network (Bristol, England)},
  keywords = {Algorithms,Color Perception,Color Perception: physiology,Depth Perception,Depth Perception: physiology,Models, Neurological,Models, Statistical,Vision, Binocular,Vision, Binocular: physiology,Visual Fields,Visual Fields: physiology,color,mscthesis},
  mendeley-tags = {color,mscthesis},
  month = aug,
  number = {3},
  pages = {191--210},
  pmid = {11014668},
  title = {{Independent component analysis applied to feature extraction from colour and stereo images.}},
  url = {<a href="http://www.ncbi.nlm.nih.gov/pubmed/11014668">http://www.ncbi.nlm.nih.gov/pubmed/11014668</a>},
  volume = {11},
  year = {2000}
}
</pre>

<a name="Taylor2010"></a><pre>
@article{<a href="references.html#Taylor2010">Taylor2010</a>,
  abstract = {We address the problem of learning good features for understanding video data. We introduce a model that learns latent representations of image sequences from pairs of successive images. The convolutional architecture of our model allows it to scale to realistic image sizes whilst using a compact parametrization. In experiments on the NORB dataset, we show our model extracts latent “flow fields” which correspond to the transformation between the pair of input frames. We also use our model to extract low-level motion features in a multi-stage architecture for action recognition, demonstrating competitive performance on both the KTH and Hollywood2 datasets.},
  author = {Taylor, GW and Fergus, Rob and LeCun, Y and Bregler, Christoph},
  file = {:home/maikel/Documents/mendeley/download(3).pdf:pdf},
  journal = {Computer Vision–ECCV 2010},
  keywords = {activity recognition,con-,mscthesis,optical flow,restricted boltzmann machines,unsupervised learning,video analysis,volutional nets},
  mendeley-tags = {mscthesis},
  title = {{Convolutional learning of spatio-temporal features}},
  url = {<a href="http://link.springer.com/chapter/10.1007/978-3-642-15567-3\_11 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.178.9267\&rep=rep1\&type=pdf">http://link.springer.com/chapter/10.1007/978-3-642-15567-3\_11 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.178.9267\&rep=rep1\&type=pdf</a>},
  year = {2010}
}
</pre>

<a name="Fukushima1980"></a><pre>
@article{<a href="references.html#Fukushima1980">Fukushima1980</a>,
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells', which show charac- teristics similar to simple cells or lower order hyper- complex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of self- organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cells of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern. 1.},
  annote = {$\backslash$begin\{itemize\}
    $\backslash$item Reiteration of self-organized by ''learning without a teacher''
    $\backslash$item Similar structure to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel.
    $\backslash$item Network structure:
      $\backslash$begin\{itemize\}
        $\backslash$item Input layer (photoreceptor array)
        $\backslash$item Cascade of modules each one with :
          $\backslash$begin\{itemize\}
            $\backslash$item S-cells: in the first layer Simple cells or lower order hypercomplex cells
            $\backslash$item C-cells: in the second layer Complex cells or higher order hypercomplex cells
          $\backslash$end\{itemize\}
      $\backslash$end\{itemize\}
    $\backslash$item Hubel and Wiesel : the neural network in the visual cortex has a hierarchy structure:
      $\backslash$begin\{itemize\}
        $\backslash$item LGB (Lageral Geniculate Body)
        $\backslash$item Simple cells
        $\backslash$item Complex cells
        $\backslash$item Lower order hypercomplex cells
        $\backslash$item Higher order hypercomplex cells
      $\backslash$end\{itemize\}
    $\backslash$item a cell in a higher stage generally has tendency to respond selectively to a more complicated feature of the stimulus pattern
    $\backslash$item we extend the hierarchy model of Hubel and Wiesel, and $\backslash$textbf\{hypothesize\} the existance of a similar hierarchy structure even in hte stages higher than hypercomplex cells.
    $\backslash$item In the last module, the receptive field of each C-cell becomes so large as to cover the whole area of input layer \$U\_0\$, and each C-plane is so determined as to have only one C-cell
    $\backslash$item The output of an S-cell in the \$k\_l\$-th S-plane in the l-th module is described below
        
$\backslash$end\{itemize\}},
  author = {Fukushima, Kunihiko},
  file = {:home/maikel/Documents/mendeley/Fukushima1980.pdf:pdf},
  keywords = {mscthesis,visual cortex},
  mendeley-tags = {mscthesis,visual cortex},
  title = {{Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position}},
  volume = {202},
  year = {1980}
}
</pre>

<a name="Ciresan2013"></a><pre>
@article{<a href="references.html#Ciresan2013">Ciresan2013</a>,
  abstract = {We use deep max-pooling convolutional neural networks to detect mitosis in breast histology images. The networks are trained to classify each pixel in the images, using as context a patch centered on the pixel. Simple postprocessing is then applied to the network output. Our approach won the ICPR 2012 mitosis detection competition, outperforming other contestants by a significant margin.},
  author = {Cireşan, DC and Giusti, Alessandro},
  file = {:home/maikel/Documents/mendeley/miccai2013.pdf:pdf},
  journal = {Medical Image \ldots},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  title = {{Mitosis detection in breast cancer histology images with deep neural networks}},
  url = {<a href="http://link.springer.com/chapter/10.1007/978-3-642-40763-5\_51 http://www.idsia.ch/~ciresan/data/miccai2013.pdf">http://link.springer.com/chapter/10.1007/978-3-642-40763-5\_51 http://www.idsia.ch/~ciresan/data/miccai2013.pdf</a>},
  year = {2013}
}
</pre>

<a name="Krizhevsky2012"></a><pre>
@article{<a href="references.html#Krizhevsky2012">Krizhevsky2012</a>,
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 millionhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\%and 17.0\% which is considerably better than the previous state-of-the-art. Theneural network, which has 60 million parameters and 650,000 neurons, consistsof five convolutional layers, some of which are followed by max-pooling layers,and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connectedlayers we employed a recently-developed regularization method called “dropout”that proved to be very effective. We also entered a variant of this model in theILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%,compared to 26.2\% achieved by the second-best entry.},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item CNN architecture:
    $\backslash$begin\{itemize\}
      $\backslash$item 650.000 neurons (60 million parameters)
      $\backslash$item 5 convolutional layers
      $\backslash$item Some of them followed by a max-pooling layer
      $\backslash$item 3 fully-connected layers
      $\backslash$item 1 1000-way softmax
    $\backslash$end\{itemize\}
  $\backslash$item Dropout regularization method to reduce overfitting in 3 fully-connected layers
  $\backslash$item Training time: 5-6 days on two GTX 580 3GB GPUs
  $\backslash$item Dataset:
    $\backslash$begin\{itemize\}
      $\backslash$item ILSVRC-2010
      $\backslash$item Down-sampled images to a fixed resolution of 256x256
      $\backslash$item Substract the mean activity ofver training set from each pixel
    $\backslash$end\{itemize\}
  $\backslash$item ReLU:
  $\backslash$begin\{itemize\}
      $\backslash$item \$f(x) = \backslash max(0,x)\$
      $\backslash$item Faster than tanh
      $\backslash$item ReLU: 6 epochs
      $\backslash$item tanh: 36 more epochs to achieve same performance
  $\backslash$end\{itemize\}
  $\backslash$item Local Response Normalization
    $\backslash$begin\{itemize\}
      $\backslash$item \$1.2\$ and \$1.4\backslash\%\$ error reduction
      $\backslash$item Helps generalization
      $\backslash$item \$b\_\{x,y\}\^{}i = a\_\{x,y\}\^{}i / \backslash left( k + \backslash alpha \backslash sum\backslash limits\_\{j=\backslash max(0,i-n/2)\}\^{}\{\backslash min(N-1,i+n/2)\}
        (a\_\{x,y\}\^{}j)\^{}2 \backslash right)\^{}\backslash beta\$
      $\backslash$item \$k=2, n=5, \backslash alpha=10\^{}-4\$, and \$\backslash beta=0.75\$
    $\backslash$end\{itemize\}
  $\backslash$item Overlapping Pooling
    $\backslash$begin\{itemize\}
      $\backslash$item \$0.3\$ and \$0.4\backslash\%\$ error reduction
      $\backslash$item grid \$3 \backslash prod 3\$
      $\backslash$item stride = 2
      $\backslash$item Overlap each pooling one column pixel
    $\backslash$end\{itemize\}
  $\backslash$item Overall Architecture
    $\backslash$begin\{itemize\}
      $\backslash$item 224x224x3 (RGB image)
      $\backslash$item Conv 96 kernels of size 11x11x3 with stride of 4 pixels
      $\backslash$item Response-Normalized and max-pooling
      $\backslash$item Conv 256 kernels of size 5x5x48 with stride of ? pixels
      $\backslash$item Response-Normalized and max-pooling
      $\backslash$item Conv 384 kernels of size 3x3x256
      $\backslash$item Conv 384 kernels of size 3x3x192
      $\backslash$item Conv 256 kernels of size 3x3x192
      $\backslash$item ¿Response-Normalized? and Max-pooling
      $\backslash$item Fully connected 4096
      $\backslash$item Fully connected 4096
      $\backslash$item Fully connected 1000
      $\backslash$item Softmax
    $\backslash$end\{itemize\}
$\backslash$figuremacro\{figures/krizhevsky2012\_convnet.pdf\}\{Architecture of the CNN\}\{\}
  $\backslash$item Data augmentation
    $\backslash$begin\{itemize\}
      $\backslash$item \$0.1\$ error reduction
      $\backslash$item Original images escaled scaled and croped to 256x256
      $\backslash$item Extract 5 images of 224x224 from corners plus center
      $\backslash$item Mirror horizontally and get 5 more images
      $\backslash$item Augment data altering RGB channels:
        $\backslash$begin\{itemize\}
          $\backslash$item Perform PCA on RGB throughout the training set
          $\backslash$item Each training image add multiples of PCs with gaussian noise
        $\backslash$end\{itemize\}
    $\backslash$end\{itemize\}
  $\backslash$item Dropout
    $\backslash$begin\{itemize\}
      $\backslash$item Put to zero the output of neurons with probability 0.5
      $\backslash$item At test time multiply the outputs by 0.5
      $\backslash$item Two first fully-connected layers
      $\backslash$item Solves overfitting
      $\backslash$item Dobules the number of iterations required to ocnverge
    $\backslash$end\{itemize\}
  $\backslash$item Details of learning
    $\backslash$begin\{itemize\}
      $\backslash$item batch size = 128
      $\backslash$item momentum 0.9
      $\backslash$item weight decay 0.0005
      $\backslash$item Initial weights from zero-mean Gaussian std=0.01
      $\backslash$item biases = 1 on second, fourth, fifth Conv and fully-connected
      $\backslash$item biases = 0 on the rest
    $\backslash$end\{itemize\}
  $\backslash$item Evaluation
    $\backslash$begin\{itemize\}
      $\backslash$item Consider the feature activations induced by an image at the last, 4096-dimensional hidden layer
    $\backslash$end\{itemize\}
$\backslash$end\{itemize\}},
  author = {Krizhevsky, Alex and Sutskever, I and Hinton, GE},
  file = {:home/maikel/Documents/mendeley/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
  journal = {NIPS},
  keywords = {imagenet,mscthesis},
  mendeley-tags = {imagenet,mscthesis},
  pages = {1--9},
  title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
  url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  year = {2012}
}
</pre>

<a name="Oneata2013"></a><pre>
@article{<a href="references.html#Oneata2013">Oneata2013</a>,
  abstract = {Action recognition in uncontrolled video is an important and challenging computer vision problem. Recent progress in this area is due to new local features and models that capture spatio-temporal structure between local features, or human-object interactions. Instead of working towards more complex models, we focus on the low-level features and their encoding. We evaluate the use of Fisher vectors as an alternative to bag-of-word histograms to aggregate a small set of state-of-the-art low-level descriptors, in combination with linear classifiers. We present a large and varied set of evaluations, considering (i) classification of short actions in five datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that for basic action recognition and localization MBH features alone are enough for state-of-the-art performance. For complex events we find that SIFT and MFCC features provide complementary cues. On all three problems we obtain state-of-the-art results, while using fewer features and less complex models.},
  author = {Oneata, D and Verbeek, Jakob and Schmid, C},
  file = {:home/maikel/Documents/mendeley/action\_and\_event\_recognition\_with\_fisher\_vectors.pdf:pdf},
  journal = {\ldots Conference on Computer \ldots},
  keywords = {mscthesis,trecvid},
  mendeley-tags = {mscthesis,trecvid},
  title = {{Action and event recognition with Fisher vectors on a compact feature set}},
  url = {<a href="http://hal.archives-ouvertes.fr/docs/00/87/36/62/PDF/action\_and\_event\_recognition\_with\_fisher\_vectors.pdf">http://hal.archives-ouvertes.fr/docs/00/87/36/62/PDF/action\_and\_event\_recognition\_with\_fisher\_vectors.pdf</a>},
  year = {2013}
}
</pre>

<a name="Weston2008"></a><pre>
@article{<a href="references.html#Weston2008">Weston2008</a>,
  abstract = {We show how nonlinear semi-supervised embedding algorithms popular for use with “shallow” learning techniques such as kernel methods can be easily applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. Compared to standard supervised backpropagation this can give significant gains. This trick provides a simple alternative to existing approaches to semi-supervised deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.},
  address = {New York, New York, USA},
  author = {Weston, Jason and Ratle, Fr\'{e}d\'{e}ric and Collobert, Ronan},
  doi = {10.1145/1390156.1390303},
  file = {:home/maikel/Documents/mendeley/deep\_embed.pdf:pdf},
  isbn = {9781605582054},
  journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
  keywords = {mscthesis,trecvid},
  mendeley-tags = {mscthesis,trecvid},
  pages = {1168--1175},
  publisher = {ACM Press},
  title = {{Deep learning via semi-supervised embedding}},
  url = {<a href="http://portal.acm.org/citation.cfm?doid=1390156.1390303 http://cse.iitk.ac.in/users/cs671/2013/hw3/weston-ratle-collobert-12\_deep-learning-via-semi-supervised-embedding.pdf">http://portal.acm.org/citation.cfm?doid=1390156.1390303 http://cse.iitk.ac.in/users/cs671/2013/hw3/weston-ratle-collobert-12\_deep-learning-via-semi-supervised-embedding.pdf</a>},
  year = {2008}
}
</pre>

<a name="Snoek2013"></a><pre>
@article{<a href="references.html#Snoek2013">Snoek2013</a>,
  abstract = {In this paper we summarize our TRECVID 2013 video retrieval experiments. The MediaMill team participated in four tasks: concept detection, object localization, in- stance search, and event recognition. For all tasks the starting point is our top-performing bag-of-words system of TRECVID 2008-2012, which uses color SIFT descrip- tors, average and difference coded into codebooks with spa- tial pyramids and kernel-based machine learning. New this year are concept detection with deep learning, concept detec- tion without annotations, object localization using selective search, instance search by reranking, and event recognition based on concept vocabularies. Our experiments focus on es- tablishing the video retrieval value of the innovations. The 2013 edition of the TRECVID benchmark has again been a fruitful participation for the MediaMill team, resulting in the best result for concept detection, concept detection with- out annotation, object localization, concept pair detection, and visual event recognition with few examples.},
  author = {Snoek, CGM and van de Sande, KEA},
  file = {:home/maikel/Documents/mendeley/mediamill.pdf:pdf},
  journal = {\ldots of TRECVID},
  keywords = {mscthesis,trecvid},
  mendeley-tags = {mscthesis,trecvid},
  title = {{MediaMill at TRECVID 2013: Searching Concepts, Objects, Instances and Events in Video}},
  url = {<a href="http://staff.science.uva.nl/~smeulder/publi.php?bibtex=SnoekPTRECVID2013">http://staff.science.uva.nl/~smeulder/publi.php?bibtex=SnoekPTRECVID2013</a>},
  year = {2013}
}
</pre>

<a name="Araujo"></a><pre>
@article{<a href="references.html#Araujo">Araujo</a>,
  abstract = {Video search has become a very important tool, with the ever-growing size of multimedia collections. This work introduces our Video Semantic Indexing system. Our experiments show that Residual Vectors provide an efficient way of aggregat- ing local descriptors, with complementary gain with respect to BoVW. Also, we show that systems using a limited number of descriptors and machine learning techniques can still be quite effective. Our first participation at the TRECVID evaluation has been very fruitful: our team was ranked 6th in the light version of the Semantic Indexing task.},
  author = {Araujo, A F De and Silveira, F and Lakshman, H and Zepeda, J and Sheth, A and Girod, B},
  file = {:home/maikel/Documents/mendeley/stanford\_trecvid.pdf:pdf},
  keywords = {bovw,centrist,dense ex-,for each run,harlap keypoint detector,l a stanford1 1,mscthesis,oppsift,residual,sift,spm,traction,trecvid},
  mendeley-tags = {mscthesis,trecvid},
  title = {{The Stanford / Technicolor / Fraunhofer HHI Video}},
  year = {2012}
}
</pre>

<a name="Le2011"></a><pre>
@article{<a href="references.html#Le2011">Le2011</a>,
  abstract = {We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70\% relative improvement over the previous state-of-the-art.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1112.6209v5},
  author = {Le, QV and Ranzato, MA and Monga, R and Devin, Matthieu},
  eprint = {arXiv:1112.6209v5},
  file = {:home/maikel/Documents/mendeley/1112.6209.pdf:pdf},
  journal = {arXiv preprint arXiv: \ldots},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  title = {{Building high-level features using large scale unsupervised learning}},
  url = {<a href="http://arxiv.org/pdf/1112.6209.pdf">http://arxiv.org/pdf/1112.6209.pdf</a>},
  year = {2011}
}
</pre>

<a name="Lee2009"></a><pre>
@article{<a href="references.html#Lee2009">Lee2009</a>,
  abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
  address = {New York, New York, USA},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item Probabilistic max-pooling
  $\backslash$item Scale DBN to real-sized images
    $\backslash$begin\{itemize\}
      $\backslash$item Computationally intractable
      $\backslash$item Need invariance in representation
    $\backslash$end\{itemize\}
  $\backslash$item RBM
    $\backslash$begin\{itemize\}
      $\backslash$item Binary valued: Independent Bernoulli random variables
      $\backslash$item Real valued: Gaussian with diagonal covariance
      $\backslash$item Training:
        $\backslash$begin\{itemize\}
          $\backslash$item Stochastic gradient ascent on log-likelihood of training data
          $\backslash$item Contrastive divergence approximation
        $\backslash$end\{itemize\}
    $\backslash$end\{itemize\}
  $\backslash$item Convolutional RBM
    $\backslash$begin\{itemize\}
      $\backslash$item detection layers: convolving feature maps
      $\backslash$item pooling layers: shrink the representation
        $\backslash$begin\{itemize\}
          $\backslash$item Block: CxC from bottom layer
          $\backslash$item Max-pooling : minimizes energy subject to only one unit can be active.
        $\backslash$end\{itemize\}
      $\backslash$item Sparsity regularization: hidden units have a mean activation close to a small constant
    $\backslash$end\{itemize\}
  $\backslash$item Convolutional Deep belief network
    $\backslash$begin\{itemize\}
      $\backslash$item  Stacking CRBM on top of one another
      $\backslash$item Training:
        $\backslash$begin\{itemize\}
          $\backslash$item Gibbs sampling
          $\backslash$item Mean-field (5 iterations in this paper)
        $\backslash$end\{itemize\}
    $\backslash$end\{itemize\}
$\backslash$end\{itemize\}
      },
  author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
  doi = {10.1145/1553374.1553453},
  file = {:home/maikel/Documents/mendeley/icml09-cdbn.pdf:pdf},
  isbn = {9781605585161},
  journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
  keywords = {CNN,mscthesis,trecvid},
  mendeley-tags = {CNN,mscthesis,trecvid},
  pages = {1--8},
  publisher = {ACM Press},
  title = {{Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations}},
  url = {<a href="http://portal.acm.org/citation.cfm?doid=1553374.1553453 http://people.csail.mit.edu/rgrosse/icml09-cdbn.pdf">http://portal.acm.org/citation.cfm?doid=1553374.1553453 http://people.csail.mit.edu/rgrosse/icml09-cdbn.pdf</a>},
  year = {2009}
}
</pre>

<a name="Sermanet2014"></a><pre>
@article{<a href="references.html#Sermanet2014">Sermanet2014</a>,
  abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item Framework for using CNN
    $\backslash$begin\{itemize\}
      $\backslash$item classification
      $\backslash$item localization
      $\backslash$item detection
    $\backslash$end\{itemize\}
  $\backslash$item Winner on localization task of ILSVRC2013
  $\backslash$item ConvNets are trained enterily with the raw pixels
  $\backslash$item Other approaches for detection and localization
  $\backslash$item appling a sliding window over multiples scales
  $\backslash$item $\backslash$dots
  $\backslash$item $\backslash$dots
$\backslash$end\{itemize\}},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1312.6229v3},
  author = {Sermanet, Pierre and Eigen, David and Zhang, X and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
  eprint = {arXiv:1312.6229v3},
  file = {:home/maikel/Documents/mendeley/1312.6229.pdf:pdf},
  journal = {arXiv preprint arXiv: \ldots},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  pages = {1--16},
  title = {{OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks}},
  url = {<a href="http://arxiv.org/abs/1312.6229 http://arxiv.org/pdf/1312.6229.pdf">http://arxiv.org/abs/1312.6229 http://arxiv.org/pdf/1312.6229.pdf</a>},
  year = {2014}
}
</pre>

<a name="Zeiler2013"></a><pre>
@article{<a href="references.html#Zeiler2013">Zeiler2013</a>,
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky et.al. on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1311.2901v3},
  author = {Zeiler, MD and Fergus, Rob},
  eprint = {arXiv:1311.2901v3},
  file = {:home/maikel/Documents/mendeley/1311.2901.pdf:pdf},
  journal = {arXiv preprint arXiv:1311.2901},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  title = {{Visualizing and Understanding Convolutional Networks}},
  url = {<a href="http://arxiv.org/abs/1311.2901 http://arxiv.org/pdf/1311.2901.pdf">http://arxiv.org/abs/1311.2901 http://arxiv.org/pdf/1311.2901.pdf</a>},
  year = {2013}
}
</pre>

<a name="Simonyan2013"></a><pre>
@article{<a href="references.html#Simonyan2013">Simonyan2013</a>,
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1312.6034v1},
  author = {Simonyan, Karen and Vedaldi, A and Zisserman, A},
  eprint = {arXiv:1312.6034v1},
  file = {:home/maikel/Documents/mendeley/1312.6034.pdf:pdf},
  journal = {arXiv preprint arXiv:1312.6034},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  pages = {1--8},
  title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
  url = {<a href="http://arxiv.org/abs/1312.6034">http://arxiv.org/abs/1312.6034</a>},
  year = {2013}
}
</pre>

<a name="Bengio2009"></a><pre>
@book{<a href="references.html#Bengio2009">Bengio2009</a>,
  abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
  author = {Bengio, Yoshua},
  booktitle = {Foundations and Trends® in Machine Learning},
  doi = {10.1561/2200000006},
  file = {:home/maikel/Documents/mendeley/Bengio - 2009 - Learning Deep Architectures for AI.pdf:pdf},
  isbn = {2200000006},
  issn = {1935-8237},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  number = {1},
  pages = {1--127},
  title = {{Learning Deep Architectures for AI}},
  url = {<a href="http://dl.acm.org/citation.cfm?id=1658424 https://wiki.eecs.yorku.ca/course\_archive/2012-13/F/6328/\_media/learning-deep-ai.pdf">http://dl.acm.org/citation.cfm?id=1658424 https://wiki.eecs.yorku.ca/course\_archive/2012-13/F/6328/\_media/learning-deep-ai.pdf</a>},
  volume = {2},
  year = {2009}
}
</pre>

<a name="LeCun1989"></a><pre>
@article{<a href="references.html#LeCun1989">LeCun1989</a>,
  abstract = {An interestmg property of connectiomst systems is their ability tolearn from examples. Although most recent work in the field concentrateson reducing learning times, the most important feature of a learning ma-chine is its generalization performance. It is usually accepted that goodgeneralization performance on real-world problems cannot be achievedunless some a pnon knowledge about the task is butlt Into the system.Back-propagation networks provide a way of specifymg such knowledgeby imposing constraints both on the architecture of the network and onits weights. In general, such constramts can be considered as particulartransformations of the parameter spaceBuilding a constramed network for image recogmtton appears to be afeasible task. We descnbe a small handwritten digit recogmtion problemand show that, even though the problem is linearly separable, single layernetworks exhibit poor generalizatton performance. Multtlayer constrainednetworks perform very well on this task when orgamzed in a hierarchicalstructure with shift invariant feature detectors.These results confirm the idea that minimizing the number of freeparameters in the network enhances generalization.},
  author = {LeCun, Y},
  file = {:home/maikel/Documents/mendeley/Unknown - Unknown - lecun-89.pdf.pdf:pdf},
  journal = {Connections in Perspective. North-Holland, \ldots},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  title = {{Generalization and network design strategies}},
  url = {<a href="http://masters.donntu.edu.ua/2012/fknt/umiarov/library/lecun.pdf">http://masters.donntu.edu.ua/2012/fknt/umiarov/library/lecun.pdf</a>},
  year = {1989}
}
</pre>

<a name="Karpathy"></a><pre>
@article{<a href="references.html#Karpathy">Karpathy</a>,
  abstract = {Convolutional Neural Networks (CNNs) have been es-tablished as a powerful class of models for image recog-nition problems. Encouraged by these results, we pro-vide an extensive empirical evaluation of CNNs on large- scale video classification using a new dataset of 1 millionYouTube videos belonging to 487 classes. We study mul-tiple approaches for extending the connectivity of a CNNin time domain to take advantage of local spatio-temporalinformation and suggest a multiresolution, foveated archi-tecture as a promising way of speeding up the training.Our best spatio-temporal networks display significant per-formance improvements compared to strong feature-basedbaselines (55.3\% to 63.9\%), but only a surprisingly mod-est improvement compared to single-frame models (59.3\%to 60.9\%). We further study the generalization performanceof our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant per-formance improvements compared to the UCF-101 baselinemodel (63.3\% up from 43.9\%).},
  annote = {$\backslash$begin\{itemize\}
    $\backslash$item Compare different CNN architectures for video classification
    $\backslash$item Create a new dataset with 1 million of YouTube sport videos and 487 classes
    $\backslash$item They required one month of training
    $\backslash$item Multiresolution CNNs: New CNN with low resolution context and high resolution center
      $\backslash$begin\{itemize\}
        $\backslash$item Context stream: seems to learn color filters
        $\backslash$item Fovea stream: learns grayscale features
      $\backslash$end\{itemize\}
    $\backslash$item Compare with and without pretraining on other dataset UCF-101
    $\backslash$item Architectures (increasing spatio-temporal relations)
      $\backslash$begin\{itemize\}
        $\backslash$item Single frame: Classify with one single shot
        $\backslash$item Late Fusion: Classify with separate-in-time shots
        $\backslash$item Early Fusion: Classify with adjacent shots merging on first convolution layer
        $\backslash$item Slow Fusion: Classify with adjacent shots progressively mergin in upper layers
      $\backslash$end\{itemize\}
    $\backslash$item Results (best models):
      $\backslash$begin\{itemize\}
        $\backslash$item clip Hit, Video Hit, Video Hit top5
        $\backslash$item 42.4 60.0 78.5 Single-Frame + Multiresolution
        $\backslash$item 41.9 60.9 80.2 Slow Fusion
      $\backslash$end\{itemize\}
    $\backslash$item Results on UCF-101 with pretraining:
      $\backslash$begin\{itemize\}
        $\backslash$item 41.3 No pretraining
        $\backslash$item 64.1 Fine-tune top layer
        $\backslash$item 65.4 Fine-tune top 3 layers
        $\backslash$item 62.2 Fine-tune all layers
      $\backslash$end\{itemize\}
    $\backslash$item Conclusions:
      $\backslash$begin\{itemize\}
        $\backslash$item From video classification can be derived that camera movements
          deteriorate the predictions
        $\backslash$item Single frame gives very good results
      $\backslash$end\{itemize\}
    $\backslash$item Further work:
      $\backslash$begin\{itemize\}
        $\backslash$item Apply some filter for camera movements
        $\backslash$item Explore RNN from clip-level into video-level
      $\backslash$end\{itemize\}
$\backslash$end\{itemize\}},
  author = {Karpathy, Andrej and Toderici, G and Shetty, S and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
  file = {:home/maikel/Documents/mendeley/karpathy14.pdf:pdf},
  journal = {vision.stanford.edu},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  title = {{Large-scale Video Classification with Convolutional Neural Networks}},
  url = {<a href="http://vision.stanford.edu/pdf/karpathy14.pdf">http://vision.stanford.edu/pdf/karpathy14.pdf</a>},
  year = {2014}
}
</pre>

<a name="Bogovic2013"></a><pre>
@article{<a href="references.html#Bogovic2013">Bogovic2013</a>,
  abstract = {For image recognition and labeling tasks, recent results suggest that machine learning methods that rely on manually specified feature representations may be outperformed by methods that automatically derive feature representations based on the data. Yet for problems that involve analysis of 3d objects, such as mesh segmentation, shape retrieval, or neuron fragment agglomeration, there remains a strong reliance on hand-designed feature descriptors. In this paper, we evaluate a large set of hand-designed 3d feature descriptors alongside features learned from the raw data using both end-to-end and unsupervised learning techniques, in the context of agglomeration of 3d neuron fragments. By combining unsupervised learning techniques with a novel dynamic pooling scheme, we show how pure learning-based methods are for the first time competitive with hand-designed 3d shape descriptors. We investigate data augmentation strategies for dramatically increasing the size of the training set, and show how combining both learned and hand-designed features leads to the highest accuracy.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1312.6159v1},
  author = {Bogovic, JA and Huang, GB and Jain, Viren},
  eprint = {arXiv:1312.6159v1},
  file = {:home/maikel/Documents/mendeley/1312.6159v1.pdf:pdf},
  journal = {arXiv preprint arXiv:1312.6159},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  pages = {1--14},
  title = {{Learned versus Hand-Designed Feature Representations for 3d Agglomeration}},
  url = {<a href="http://arxiv.org/abs/1312.6159">http://arxiv.org/abs/1312.6159</a>},
  year = {2013}
}
</pre>

<a name="Bruna2014"></a><pre>
@article{<a href="references.html#Bruna2014">Bruna2014</a>,
  abstract = {Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possi- ble generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low- dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1312.6203v2},
  author = {Bruna, Joan and Szlam, Arthur and Zaremba, Wojciech and LeCun, Yann},
  eprint = {arXiv:1312.6203v2},
  file = {:home/maikel/Documents/mendeley/1312.6203v2.pdf:pdf},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  pages = {1--14},
  title = {{Spectral Networks and Deep Locally Connected Networks on Graphs}},
  year = {2014}
}
</pre>

<a name="Reddy2012"></a><pre>
@article{<a href="references.html#Reddy2012">Reddy2012</a>,
  abstract = {Action recognition on large categories of unconstrained videos taken from the web is a very challenging problem compared to datasets like KTH (6 actions), IXMAS (13 actions), and Weizmann (10 actions). Challenges like camera motion, different viewpoints, large interclass variations, cluttered background, occlusions, bad illumination conditions, and poor quality of web videos cause the majority of the state-of-the-art action recognition approaches to fail. Also, an increased number of categories and the inclusion of actions with high confusion add to the challenges. In this paper, we propose using the scene context information obtained from moving and stationary pixels in the key frames, in conjunction with motion features, to solve the action recognition problem on a large (50 actions) dataset with videos from the web. We perform a combination of early and late fusion on multiple features to handle the very large number of categories. We demonstrate that scene context is a very important feature to perform action recognition on very large datasets. The proposed method does not require any kind of video stabilization, person detection, or tracking and pruning of features. Our approach gives good performance on a large number of action categories; it has been tested on the UCF50 dataset with 50 action categories, which is an extension of the UCF YouTube Action (UCF11) dataset containing 11 action categories. We also tested our approach on the KTH and HMDB51 datasets for comparison.},
  author = {Reddy, Kishore K. and Shah, Mubarak},
  doi = {10.1007/s00138-012-0450-4},
  file = {:home/maikel/Documents/mendeley/MVAP\_UCF50.pdf:pdf},
  issn = {0932-8092},
  journal = {Machine Vision and Applications},
  keywords = {action recognition,fusion,mscthesis,web videos},
  mendeley-tags = {mscthesis},
  month = nov,
  number = {5},
  pages = {971--981},
  title = {{Recognizing 50 human action categories of web videos}},
  url = {<a href="http://link.springer.com/10.1007/s00138-012-0450-4">http://link.springer.com/10.1007/s00138-012-0450-4</a>},
  volume = {24},
  year = {2012}
}
</pre>

<a name="Hubel1962"></a><pre>
@article{<a href="references.html#Hubel1962">Hubel1962</a>,
  author = {Hubel, DH and Wiesel, TN},
  file = {:home/maikel/Documents/mendeley/jphysiol01247-0121.pdf:pdf},
  journal = {The Journal of physiology},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  pages = {106--154},
  title = {{Receptive fields, binocular interaction and functional architecture in the cat's visual cortex}},
  url = {<a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/pdf/jphysiol01247-0121.pdf">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/pdf/jphysiol01247-0121.pdf</a>},
  year = {1962}
}
</pre>

<a name="Hyvarinen2000"></a><pre>
@article{<a href="references.html#Hyvarinen2000">Hyvarinen2000</a>,
  abstract = {Olshausen and Field (1996) applied the principle of independence maximization by sparse coding to extract features from natural images. This leads to the emergence of oriented linear filters that have simultaneous localization in space and in frequency, thus resembling Gabor functions and simple cell receptive fields. In this article, we show that the same principle of independence maximization can explain the emergence of phase- and shift-invariant features, similar to those found in complex cells. This new kind of emergence is obtained by maximizing the independence between norms of projections on linear subspaces (instead of the independence of simple linear filter outputs). The norms of the projections on such “independent feature subspaces” then indicate the values of invariant features.},
  author = {Hyv\"{a}rinen, A and Hoyer, Patrik},
  file = {:home/maikel/Documents/mendeley/089976600300015312.pdf:pdf},
  journal = {Neural computation},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  pages = {1705--1720},
  title = {{Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces}},
  url = {<a href="http://www.mitpressjournals.org/doi/abs/10.1162/089976600300015312">http://www.mitpressjournals.org/doi/abs/10.1162/089976600300015312</a>},
  volume = {1720},
  year = {2000}
}
</pre>

<a name="Wang2009"></a><pre>
@article{<a href="references.html#Wang2009">Wang2009</a>,
  abstract = {Local space-time features have recently become a popular video representation for action recognition. Several methods for feature localization and description have been proposed in the literature and promising recognition results were demonstrated for a number of action classes. The comparison of existing methods, however, is often limited given the different experimental settings used. The purpose of this paper is to evaluate and compare previously proposed space-time features in a common experimental setup. In particular, we consider four different feature detectors and six local feature descriptors and use a standard bag-of-features SVM approach for action recognition. We investigate the performance of these methods on a total of 25 action classes distributed over three datasets with varying difficulty. Among interesting conclusions, we demonstrate that regular sampling of space-time features consistently outperforms all tested space-time interest point detectors for human actions in realistic settings. We also demonstrate a consistent ranking for the majority of methods over different datasets and discuss their advantages and limitations.},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item Detectors
    $\backslash$begin\{itemize\}
      $\backslash$item Harris3D
      $\backslash$item Cuboid
      $\backslash$item Hessian
      $\backslash$item Dense sampling
    $\backslash$end\{itemize\}
  $\backslash$item Descriptors
    $\backslash$begin\{itemize\}
      $\backslash$item HOG/HOF
      $\backslash$item HOG3D
      $\backslash$item ESURF (extended SURF)
    $\backslash$end\{itemize\}
  $\backslash$item Datasets
    $\backslash$begin\{itemize\}
      $\backslash$item KTH actions
        $\backslash$begin\{itemize\}
          $\backslash$item 6 human action classes
          $\backslash$item walking, jogging, running, boxing, waving and clapping
          $\backslash$item 25 subjects
          $\backslash$item 4 scenarios
          $\backslash$item 2391 video samples
          $\backslash$item $\backslash$url\{http://www.nada.kth.se/cvap/actions/\}
        $\backslash$end\{itemize\}
      $\backslash$item UCF sport actions
        $\backslash$begin\{itemize\}
          $\backslash$item 10 human action classes
          $\backslash$item winging, diving, kicking, weight-lifting, horse-riding,
            running, skateboarding, swinging, golf swinging and walking
          $\backslash$item 150 video samples
          $\backslash$item $\backslash$url\{http://crcv.ucf.edu/data/UCF\_Sports\_Action.php\}
        $\backslash$end\{itemize\}
      $\backslash$item Hollywood2 actions
        $\backslash$begin\{itemize\}
          $\backslash$item 12 action classes
          $\backslash$item answering the hone, driving car, eating, fighting,
            geting out of the car, hand shaking, hugging, kissing,
            running, sitting down, sitting up, and standing up.
          $\backslash$item 69 Hollywood movies
          $\backslash$item 1707 video samples
          $\backslash$item $\backslash$url\{http://www.di.ens.fr/\~{}laptev/actions/hollywood2/\}
        $\backslash$end\{itemize\}
    $\backslash$end\{itemize\}
$\backslash$end\{itemize\}},
  author = {Wang, Heng and Ullah, Muhammad Muneeb and Klaser, Alexander and Laptev, Ivan and Schmid, Cordelia},
  doi = {10.5244/C.23.124},
  file = {:home/maikel/Documents/mendeley/paper.pdf:pdf},
  isbn = {1-901725-39-1},
  journal = {Procedings of the British Machine Vision Conference 2009},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  pages = {124.1--124.11},
  publisher = {British Machine Vision Association},
  title = {{Evaluation of local spatio-temporal features for action recognition}},
  url = {<a href="http://www.bmva.org/bmvc/2009/Papers/Paper143/Paper143.html">http://www.bmva.org/bmvc/2009/Papers/Paper143/Paper143.html</a>},
  year = {2009}
}
</pre>

<a name="Norouzi2009"></a><pre>
@article{<a href="references.html#Norouzi2009">Norouzi2009</a>,
  abstract = {In this paper we present a method for learning class-specific features for recognition. Recently a greedy layer-wise procedure was proposed to initialize weights of deep belief networks, by viewing each layer as a separate restricted Boltzmann machine (RBM). We develop the convolutional RBM (C-RBM), a variant of the RBM model in which weights are shared to respect the spatial structure of images. This framework learns a set of features that can generate the images of a specific object class. Our feature extraction model is a four layer hierarchy of alternating filtering and maximum subsampling. We learn feature parameters of the first and third layers viewing them as separate C-RBMs. The outputs of our feature extraction hierarchy are then fed as input to a discriminative classifier. It is experimentally demonstrated that the extracted features are effective for object detection, using them to obtain performance comparable to the state of the art on handwritten digit recognition and pedestrian detection.},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item New Convolutional Restricted Boltzmann Machine (C-RBM)
  $\backslash$item Comparable state-of-the-art on handwritten digit recognition and pedestrian detection
  $\backslash$item RBM
    $\backslash$begin\{itemize\}
      $\backslash$item Probabilistic model
      $\backslash$item hidden variables independent given observerd data
      $\backslash$item Not capture explicitly spacial structure of images
    $\backslash$end\{itemize\}
  $\backslash$item C-RBM
    $\backslash$begin\{itemize\}
      $\backslash$item Include spatial locality and weight sharing
      $\backslash$item Favors filters with high response on training images
      $\backslash$item Unsupervised learning using Contrastive Divergence
      $\backslash$item Layerwise training for stacks of RBMs
      $\backslash$item Convolutional connections are employed in a generative Markov Random Field architecture
      $\backslash$item Hidden units divided into K feature maps
      $\backslash$item Convolution problems
        $\backslash$begin\{itemize\}
          $\backslash$item Boundary units are withinb a smaller number of subwindows compared to the interior pixels
          $\backslash$item middle pixels may contribute to \$K\_\{xy\}\$ features
          $\backslash$item Separation of boundary variables (\$v\^{}b\$) from middle variables (\$v\^{}m\$)
          $\backslash$item Problems sampling from boundary pixels (not have nough features)
          $\backslash$item Over completeness because of K-features
          $\backslash$item Sampling creates images very similar to the original ones
          $\backslash$item Need of more Gibbs sampling steps
          $\backslash$item Their solution is to fix hidden bias terms \$c\$ during training
        $\backslash$end\{itemize\}
    $\backslash$end\{itemize\}
  $\backslash$item Multilayer C-RBMs
    $\backslash$begin\{itemize\}
      $\backslash$item Subsampling takes maximum conditional feature probability over non-overlapping subwindows of feature maps
      $\backslash$item Architecture
        $\backslash$begin\{itemize\}
          $\backslash$item discriminative layer (SVM)
          $\backslash$item max pooling
          $\backslash$item convolution
          $\backslash$item max pooling
          $\backslash$item convolution
          $\backslash$item input
        $\backslash$end\{itemize\}
      $\backslash$item On pedestrians also HOG is used in discriminative layer
    $\backslash$end\{itemize\}
  $\backslash$item MNIST dataset
    $\backslash$begin\{itemize\}
      $\backslash$item Discriminative layer with RBF kernel
      $\backslash$item 10 one-vs-rest binary SVMs
      $\backslash$item 1st layer 15 feature maps
      $\backslash$item 2nd layer 2x2 non-overlapping subwindos
      $\backslash$item 3rd layer 15 feature maps
      $\backslash$item 4th layer
    $\backslash$end\{itemize\}
  $\backslash$item Comparison with Large CNN
    $\backslash$begin\{itemize\}
      $\backslash$item C-RBM is better when training is small
    $\backslash$end\{itemize\}
  $\backslash$item Pedestrian dataset
    $\backslash$begin\{itemize\}
      $\backslash$item 1st layer 7x7 15 feature maps
      $\backslash$item 2nd layer 4x4 subsampling
      $\backslash$item 3rd layer 15x5x5 30 feature maps
      $\backslash$item 4th layer 2x2 subsampling
      $\backslash$item + HOG
      $\backslash$item Discriminative layer with linear kernel
    $\backslash$end\{itemize\}
$\backslash$end\{itemize\}},
  author = {Norouzi, Mohammad and Ranjbar, Mani and Mori, Greg},
  file = {:home/maikel/Documents/mendeley/05206577.pdf:pdf},
  isbn = {9781424439911},
  journal = {Computer Vision and Pattern \ldots},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  pages = {2735--2742},
  title = {{Stacks of convolutional restricted Boltzmann machines for shift-invariant feature learning}},
  url = {<a href="http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5206577">http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5206577</a>},
  year = {2009}
}
</pre>

<a name="Krizhevsky2010"></a><pre>
@article{<a href="references.html#Krizhevsky2010">Krizhevsky2010</a>,
  abstract = {We describe how to train a two-layer convolutional Deep Belief Network (DBN) on the 1.6 million tiny imagesdataset.When training a convolutional DBN, one must decide what to do with the edge pixels of teh images. Asthe pixels near the edge of an image contribute to the fewest convolutional filter outputs, the model maysee it fit to tailor its few convolutional filters to better model the edge pixels. This is undesirable becaue itusually comes at the expense of a good model for the interior parts of the image. We investigate several waysof dealing with the edge pixels when training a convolutional DBN. Using a combination of locally-connectedconvolutional units and globally-connected units, as well as a few tricks to reduce the effects of overfitting,we achieve state-of-the-art performance in the classification task of the CIFAR-10 subset of the tiny imagesdataset.},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item Detectors
    $\backslash$begin\{itemize\}
      $\backslash$item Harris3D
      $\backslash$item Cuboid
      $\backslash$item Hessian
      $\backslash$item Dense sampling
    $\backslash$end\{itemize\}
  $\backslash$item Descriptors
    $\backslash$begin\{itemize\}
      $\backslash$item HOG/HOF
      $\backslash$item HOG3D
      $\backslash$item ESURF (extended SURF)
    $\backslash$end\{itemize\}
  $\backslash$item Datasets
    $\backslash$begin\{itemize\}
      $\backslash$item KTH actions
        $\backslash$begin\{itemize\}
          $\backslash$item 6 human action classes
          $\backslash$item walking, jogging, running, boxing, waving and clapping
          $\backslash$item 25 subjects
          $\backslash$item 4 scenarios
          $\backslash$item 2391 video samples
          $\backslash$item \$http://www.nada.kth.se/cvap/actions/\$
        $\backslash$end\{itemize\}
      $\backslash$item UCF sport actions
        $\backslash$begin\{itemize\}
          $\backslash$item 10 human action classes
          $\backslash$item winging, diving, kicking, weight-lifting, horse-riding,
            running, skateboarding, swinging, golf swinging and walking
          $\backslash$item 150 video samples
          $\backslash$item \$http://crcv.ucf.edu/data/UCF\_Sports\_Action.php\$
        $\backslash$end\{itemize\}
      $\backslash$item Hollywood2 actions
        $\backslash$begin\{itemize\}
          $\backslash$item 12 action classes
          $\backslash$item answering the hone, driving car, eating, fighting,
            geting out of the car, hand shaking, hugging, kissing,
            running, sitting down, sitting up, and standing up.
          $\backslash$item 69 Hollywood movies
          $\backslash$item 1707 video samples
          $\backslash$item \$http://www.di.ens.fr/\~{}laptev/actions/hollywood2/\$
        $\backslash$end\{itemize\}
    $\backslash$end\{itemize\}
$\backslash$end\{itemize\}},
  author = {Krizhevsky, Alex},
  file = {:home/maikel/Documents/mendeley/download (2).pdf:pdf},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  pages = {1--9},
  title = {{Convolutional Deep Belief Networks on CIFAR-10}},
  url = {<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.5826\&rep=rep1\&type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.5826\&rep=rep1\&type=pdf</a>},
  year = {2010}
}
</pre>

<a name="Simard2003"></a><pre>
@article{<a href="references.html#Simard2003">Simard2003</a>,
  abstract = {Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple “do-it-yourself” implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependent learning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages.},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item Get a training set as large as possible
  $\backslash$item No need of complex methods, such as momentum, weight decay,
    structure-dependent learning rates, averaging layers, tangent prop, or
    even finely-tuning the architecture
  $\backslash$item Increment dataset by:
    $\backslash$begin\{itemize\}
      $\backslash$item Affine transformations: translations, scaling,
            homothety, similarity transformation, reflection, rotation, shear
            mapping, and compositions.
      $\backslash$item Elastic distortions
    $\backslash$end\{itemize\}
  $\backslash$item In this paper the authors justify the use of elastic deformations
        on MNIST data corresponding to uncontrolled oscillations of the hand muscles,
          dampened by inertia.
  $\backslash$item They get the best results on MNIST to date with CNN, affine
    and elastic transformations of the dataset (0.4$\backslash$\% error).
$\backslash$end\{itemize\}},
  author = {Simard, P and Steinkraus, Dave and Platt, JC},
  file = {:home/maikel/Documents/mendeley/ICDAR03.pdf:pdf},
  journal = {ICDAR},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  title = {{Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis}},
  url = {<a href="http://vnlab.ce.sharif.ir/courses/85-86/2/ce667/resources/root/15 - Convolutional N. N./ICDAR03.pdf">http://vnlab.ce.sharif.ir/courses/85-86/2/ce667/resources/root/15 - Convolutional N. N./ICDAR03.pdf</a>},
  year = {2003}
}
</pre>

<a name="King2002"></a><pre>
@article{<a href="references.html#King2002">King2002</a>,
  abstract = {It seems that everywhere you look there is some article or discussion about color management. Why all the fuss? Do I need to management my colors? We have been creating colored artifacts for a very long time and I don't think we have needed color management. So why now? Most of these discussions also refer to the ICC. What is that? These and other questions will be answered in a straightforward manner in plain English. Adobe Systems has pioneered the use of desktop computers for color work, and the author has helped Adobe pick its way down conflicting color paths with confusing road signs over the last 10 years.},
  author = {King, JC},
  file = {:home/maikel/Documents/mendeley/whycolormanagement.pdf:pdf},
  journal = {9th Congress of the International Color \ldots},
  keywords = {color,mscthesis},
  mendeley-tags = {color,mscthesis},
  title = {{Why color management?}},
  url = {<a href="http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=886641 https://noppa.aalto.fi/noppa/kurssi/t-75.2122/harjoitustyot/T-75\_2122\_why\_color\_management.pdf">http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=886641 https://noppa.aalto.fi/noppa/kurssi/t-75.2122/harjoitustyot/T-75\_2122\_why\_color\_management.pdf</a>},
  year = {2002}
}
</pre>

<a name="Serre2007"></a><pre>
@article{<a href="references.html#Serre2007">Serre2007</a>,
  abstract = {We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex.},
  author = {Serre, Thomas and Wolf, Lior and Bileschi, Stanley and Riesenhuber, Maximilian and Poggio, Tomaso},
  doi = {10.1109/TPAMI.2007.56},
  file = {:home/maikel/Documents/mendeley/serre-wolf-poggio-PAMI-07.pdf:pdf},
  issn = {0162-8828},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  keywords = {Algorithms,Artificial Intelligence,Biomimetics,Biomimetics: methods,Computer Simulation,Humans,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Models, Biological,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Pattern Recognition, Visual,Pattern Recognition, Visual: physiology,Reproducibility of Results,Sensitivity and Specificity,Visual Cortex,Visual Cortex: physiology,mscthesis,visual cortex},
  mendeley-tags = {mscthesis,visual cortex},
  month = mar,
  number = {3},
  pages = {411--26},
  pmid = {17224612},
  title = {{Robust object recognition with cortex-like mechanisms.}},
  url = {<a href="http://www.ncbi.nlm.nih.gov/pubmed/17224612">http://www.ncbi.nlm.nih.gov/pubmed/17224612</a>},
  volume = {29},
  year = {2007}
}
</pre>

<a name="Hubel1968"></a><pre>
@article{<a href="references.html#Hubel1968">Hubel1968</a>,
  abstract = {1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.},
  author = {Hubel, DH and Wiesel, TN},
  file = {:home/maikel/Documents/mendeley/J Physiol-1968-Hubel-215-43.pdf:pdf},
  journal = {The Journal of physiology},
  keywords = {mscthesis,visual cortex},
  mendeley-tags = {mscthesis,visual cortex},
  pages = {215--243},
  title = {{Receptive fields and functional architecture of monkey striate cortex}},
  url = {<a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/pdf/jphysiol01247-0121.pdf">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/pdf/jphysiol01247-0121.pdf</a>},
  year = {1968}
}
</pre>

<a name="LeCun1998"></a><pre>
@article{<a href="references.html#LeCun1998">LeCun1998</a>,
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item LeNet-5
  $\backslash$item Clarification: In this paper ``stride'' is not mentioned, but as Krizhevsky2012
    et.al. started using it, new implementations of CNN need to define its value.
  $\backslash$item Conv: Convolutional layer
  $\backslash$item Subs: Subsampling layer (summed * coefficient + bias)
  $\backslash$item Full: Fully connected network
  $\backslash$item ERBF: Euclidian Radial Basis Function units
    $\backslash$begin\{itemize\}
      $\backslash$item input 32x32 pixel image (original images are 28x28)
      $\backslash$item Conv1 :
        $\backslash$begin\{itemize\}
          $\backslash$item 6@28x28 filter 5x5
          $\backslash$item stride 1
          $\backslash$item Connections = \$5*5*28*28*6 + 6*28*28 = 122,304\$
          $\backslash$item Train. param. = \$5*5*6 + 6 = 156\$
        $\backslash$end\{itemize\}
      $\backslash$item Subs2 :
        $\backslash$begin\{itemize\}
          $\backslash$item 6@14x14 range 2x2
          $\backslash$item stride 2
          $\backslash$item Connections = \$6*28*28 + 6*14*14 = 5,880\$
          $\backslash$item Train. param. = coefficient + bias = \$6 + 6 = 156\$
        $\backslash$end\{itemize\}
      $\backslash$item Conv3 :
        $\backslash$begin\{itemize\}
          $\backslash$item 16@10x10 filter 5x5
          $\backslash$item stride 1
          $\backslash$item Connections \$= 6*5*5*10*10*10 + 10*10*16 = 151,600\$
          $\backslash$item Train. param. \$= 5*5*3*6 + 5*5*4*9 + 5*5*6*1 + 16 = 1,516\$
          $\backslash$item Note:
          $\backslash$item This layer is not completly connected, see table 1 for specific connections
          $\backslash$item Expected Connections \$= 6*5*5*10*10*16 + 10*10*16 = 241,600\$
          $\backslash$item Expected train. param \$= 5*5*16*6 + 16 = 2416\$
        $\backslash$end\{itemize\}
      $\backslash$item Subs4 :
        $\backslash$begin\{itemize\}
          $\backslash$item 16@5x5 range 2x2
          $\backslash$item stride 2
          $\backslash$item Connections \$= 16*10*10 + 16*5*5 = 2,000\$
          $\backslash$item Train. param. = coefficient + bias \$= 16 + 16 = 32\$
        $\backslash$end\{itemize\}
      $\backslash$item Conv5 :
        $\backslash$begin\{itemize\}
          $\backslash$item 120@1x1 filter 5x5
          $\backslash$item stride 0
          $\backslash$item Connections and train. param. \$= 16*5*5*120 + 120 = 48,120\$
        $\backslash$end\{itemize\}
      $\backslash$item Full6 : 84 Atanh(Sa)
        $\backslash$begin\{itemize\}
          $\backslash$item Connections and train. param. \$= 120*84 + 84 = 10,164\$
        $\backslash$end\{itemize\}
      $\backslash$item ERBF7 : 10
        $\backslash$begin\{itemize\}
          $\backslash$item Connections and train. param. = \$84*10 = 840\$
        $\backslash$end\{itemize\}
    $\backslash$end\{itemize\}
$\backslash$end\{itemize\}},
  author = {LeCun, Y and Bottou, L},
  file = {:home/maikel/Documents/mendeley/lecun-01a (1).pdf:pdf},
  journal = {Proceedings of the \ldots},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  title = {{Gradient-based learning applied to document recognition}},
  url = {<a href="http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=726791 http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=726791 http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf</a>},
  year = {1998}
}
</pre>

<a name="Kavukcuoglu2010"></a><pre>
@article{<a href="references.html#Kavukcuoglu2010">Kavukcuoglu2010</a>,
  abstract = {We propose an unsupervised method for learning multi-stage hierarchies of sparseconvolutional features. While sparse coding has become an increasingly popularmethod for learning visual features, it is most often trained at the patch level.Applying the resulting filters convolutionally results in highly redundant codesbecause overlapping patches are encoded in isolation. By training convolutionallyover large image windows, our method reduces the redudancy between featurevectors at neighboring locations and improves the efficiency of the overall repre-sentation. In addition to a linear decoder that reconstructs the image from sparsefeatures, our method trains an efficient feed-forward encoder that predicts quasi-sparse features from the input. While patch-based training rarely produces any-thing but oriented edge detectors, we show that convolutional training produceshighly diverse filters, including center-surround filters, corner detectors, cross de-tectors, and oriented grating detectors. We show that using these filters in multi-stage convolutional network architecture improves performance on a number ofvisual recognition and detection tasks},
  author = {Kavukcuoglu, Koray and Sermanet, Pierre and Boureau, Y-lan and LeCun, Yann and Gregor, Karol and Mathieu, Micha\"{e}l},
  file = {:home/maikel/Documents/mendeley/4133-learning-convolutional-feature-hierarchies-for-visual-recognition.pdf:pdf},
  journal = {NIPS},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  number = {1},
  pages = {1--9},
  title = {{Learning Convolutional Feature Hierarchies for Visual Recognition}},
  url = {https://papers.nips.cc/paper/4133-learning-convolutional-feature-hierarchies-for-visual-recognition.pdf},
  year = {2010}
}
</pre>

<a name="Callet2006"></a><pre>
@article{<a href="references.html#Callet2006">Callet2006</a>,
  abstract = {This paper describes an application of neural networks in the field of objective measurement method designed to automatically assess the perceived quality of digital videos. This challenging issue aims to emulate human judgment and to replace very complex and time consuming subjective quality assessment. Several metrics have been proposed in literature to tackle this issue. They are based on a general framework that combines different stages, each of them addressing complex problems. The ambition of this paper is not to present a global perfect quality metric but rather to focus on an original way to use neural networks in such a framework in the context of reduced reference (RR) quality metric. Especially, we point out the interest of such a tool for combining features and pooling them in order to compute quality scores. The proposed approach solves some problems inherent to objective metrics that should predict subjective quality score obtained using the single stimulus continuous quality evaluation (SSCQE) method. This latter has been adopted by video quality expert group (VQEG) in its recently finalized reduced referenced and no reference (RRNR-TV) test plan. The originality of such approach compared to previous attempts to use neural networks for quality assessment, relies on the use of a convolutional neural network (CNN) that allows a continuous time scoring of the video. Objective features are extracted on a frame-by-frame basis on both the reference and the distorted sequences; they are derived from a perceptual-based representation and integrated along the temporal axis using a time-delay neural network (TDNN). Experiments conducted on different MPEG-2 videos, with bit rates ranging 2-6 Mb/s, show the effectiveness of the proposed approach to get a plausible model of temporal pooling from the human vision system (HVS) point of view. More specifically, a linear correlation criteria, between objective and subjective scoring, up to 0.92 has been obtained on a- - set of typical TV videos},
  author = {Callet, P Le},
  file = {:home/maikel/Documents/mendeley/A\_convolutional\_neural\_network\_approach\_for\_objective\_video\_quality\_assessment\_completefinal\_manuscript.pdf:pdf},
  journal = {Neural Networks, IEEE \ldots},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  pages = {1316--1327},
  title = {{A convolutional neural network approach for objective video quality assessment}},
  url = {<a href="http://medcontent.metapress.com/index/A65RM03P4874243N.pdf http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1687939 http://hal.univ-nantes.fr/docs/00/28/74/26/PDF/A\_convolutional\_neural\_network\_approach\_for\_objective\_video\_quality\_assessment\_completefinal\_manuscript.pdf">http://medcontent.metapress.com/index/A65RM03P4874243N.pdf http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1687939 http://hal.univ-nantes.fr/docs/00/28/74/26/PDF/A\_convolutional\_neural\_network\_approach\_for\_objective\_video\_quality\_assessment\_completefinal\_manuscript.pdf</a>},
  volume = {5},
  year = {2006}
}
</pre>

<a name="Masci2011"></a><pre>
@article{<a href="references.html#Masci2011">Masci2011</a>,
  abstract = {We present a novel convolutional auto-encoder (CAE) for unsupervised feature learning. A stack of CAEs forms a convolutional neural network (CNN). Each CAE is trained using conventional on-line gradient descent without additional regularization terms. A max-pooling layer is essential to learn biologically plausible features consistent with those found by previous approaches. Initializing a CNN with filters of a trained CAE stack yields superior performance on a digit (MNIST) and an object recognition (CIFAR10) benchmark.},
  author = {Masci, Jonathan and Meier, Ueli and Cireşan, D and Schmidhuber, J},
  file = {:home/maikel/Documents/mendeley/2011\_icann.pdf:pdf},
  journal = {Artificial Neural Networks \ldots},
  keywords = {CNN,auto-encoder,classification,convolutional neural network,learning,mscthesis,trecvid,unsupervised},
  mendeley-tags = {CNN,mscthesis,trecvid},
  pages = {52--59},
  title = {{Stacked convolutional auto-encoders for hierarchical feature extraction}},
  url = {<a href="http://link.springer.com/chapter/10.1007/978-3-642-21735-7\_7 http://www.idsia.ch/~masci/papers/2011\_icann.pdf">http://link.springer.com/chapter/10.1007/978-3-642-21735-7\_7 http://www.idsia.ch/~masci/papers/2011\_icann.pdf</a>},
  year = {2011}
}
</pre>

<a name="Yang2013"></a><pre>
@article{<a href="references.html#Yang2013">Yang2013</a>,
  abstract = {We propose a novel approach to boost the performance of generic object detectors on videos by learning video-specific features using a deep neural network. The insight behind our proposed approach is that an object appearing in different frames of a video clip should share similar features, which can be learned to build better detectors. Unlike many supervised detector adaptation or detection-by-tracking methods, our method does not require any extra annotations or utilize temporal correspondence. We start with the high-confidence detections from a generic detector, then iteratively learn new video-specific features and refine the detection scores. In order to learn discriminative and compact features, we propose a new feature learning method using a deep neural network based on auto en-coders. It differs from the existing unsupervised feature learning methods in two ways: first it optimizes both discriminative and generative properties of the features simultaneously, which gives our features better discriminative ability, second, our learned features are more compact, while the unsupervised feature learning methods usually learn a redundant set of over-complete features. Extensive experimental results on person and horse detection show that significant performance improvement can be achieved with our proposed method.},
  author = {Yang, Yang and Shu, Guang and Shah, Mubarak},
  doi = {10.1109/CVPR.2013.216},
  file = {:home/maikel/Documents/mendeley/CVPR2013\_Yang\_FinalVersion\_HumanDetection.pdf:pdf},
  isbn = {978-0-7695-4989-7},
  journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
  keywords = {mscthesis,trecvid},
  mendeley-tags = {mscthesis,trecvid},
  month = jun,
  pages = {1650--1657},
  publisher = {Ieee},
  title = {{Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video}},
  url = {<a href="http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619060 http://crcv.ucf.edu/papers/cvpr2013/CVPR2013\_Yang\_FinalVersion\_HumanDetection.pdf">http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619060 http://crcv.ucf.edu/papers/cvpr2013/CVPR2013\_Yang\_FinalVersion\_HumanDetection.pdf</a>},
  year = {2013}
}
</pre>

<a name="Over2013"></a><pre>
@article{<a href="references.html#Over2013">Over2013</a>,
  author = {Over, Paul and Awad, George and Fiscus, Jon and Sanders, Greg},
  file = {:home/maikel/Documents/mendeley/tv13intro.pdf:pdf},
  keywords = {mscthesis,trecvid},
  mendeley-tags = {mscthesis,trecvid},
  title = {{TRECVID 2013 – An Introduction to the Goals , Tasks , Data , Evaluation Mechanisms , and Metrics}},
  year = {2013}
}
</pre>

<a name="Safadi2013"></a><pre>
@article{<a href="references.html#Safadi2013">Safadi2013</a>,
  abstract = {The Quaero group is a consortium of French and German organizations working on Multimedia Indexing and Retrieval. LIG, INRIA and KIT participated to the semantic indexing task and LIG participated to the organization of this task. This paper describes these participations. For the semantic indexing task, our approach uses a six-stages processing pipelines for computing scores for the likelihood of a video shot to contain a target concept. These scores are then used for producing a ranked list of images or shots that are the most likely to contain the target concept. The pipeline is composed of the following steps: descriptor extraction, descriptor optimization, classi cation, fusion of descriptor variants, higher-level fusion, and re-ranking. We used a number of di erent descriptors and a hierarchical fusion strategy. We also used conceptual feedback by adding a vector of classi cation score to the pool of descriptors. The best Quaero run has a Mean Inferred Average Precision of 0.2692, which ranked us 3rd out of 16 participants. We also organized the TRECVid SIN 2012 collaborative annotation.},
  author = {Safadi, Bahjat and Derbas, Nadia and Hamadi, Abdelkader and Vuong, Thi-thu-thuy and Dong, Han and Mulhem, Philippe and Qu, Georges},
  file = {:home/maikel/Documents/mendeley/tv13\_Quaero.pdf:pdf},
  keywords = {mscthesis,trecvid},
  mendeley-tags = {mscthesis,trecvid},
  title = {{Quaero at TRECVid 2013 : Semantic Indexing}},
  url = {<a href="http://hal.archives-ouvertes.fr/docs/00/77/02/40/PDF/Safadi-al\_TRECVID2012.pdf">http://hal.archives-ouvertes.fr/docs/00/77/02/40/PDF/Safadi-al\_TRECVID2012.pdf</a>},
  year = {2013}
}
</pre>

<a name="Ngiam2010"></a><pre>
@article{<a href="references.html#Ngiam2010">Ngiam2010</a>,
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to manytasks such as digit and object recognition. Using convolutional (tied) weightssignificantly reduces the number of parameters that have to be learned, and alsoallows translational invariance to be hard-coded into the architecture. In this pa-per, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolution neural networks (Tiled CNNs), which usea regular “tiled” pattern of tied weights that does not require that adjacent hiddenunits share identical weights, but instead requires only that hidden units k stepsaway from each other to have tied weights. By pooling over neighboring units,this architecture is able to learn complex invariances (such as scale and rotationalinvariance) beyond translational invariance. Further, it also enjoys much of CNNs’advantage of having a relatively small number of learned parameters (such as easeof learning and greater scalability). We provide an efficient learning algorithm forTiled CNNs based on Topographic ICA, and show that learning complex invariantfeatures allows us to achieve highly competitive results for both the NORB andCIFAR-10 datasets.},
  author = {Ngiam, Jiquan and Chen, Zhenghao and Chia, Daniel and Koh, Pan Wei and Ng, Andrew Y.},
  file = {:home/maikel/Documents/mendeley/NIPS2010\_0550.pdf:pdf},
  journal = {Advances in Neural \ldots},
  keywords = {CNN,convolutional neural networks,mscthesis},
  mendeley-tags = {CNN,convolutional neural networks,mscthesis},
  pages = {1--9},
  title = {{Tiled convolutional neural networks}},
  url = {<a href="http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2010\_0550.pdf http://papers.nips.cc/paper/4136-tiled-convolutional-neural-networks.pdf">http://machinelearning.wustl.edu/mlpapers/paper\_files/NIPS2010\_0550.pdf http://papers.nips.cc/paper/4136-tiled-convolutional-neural-networks.pdf</a>},
  year = {2010}
}
</pre>

<a name="Eigen2013"></a><pre>
@article{<a href="references.html#Eigen2013">Eigen2013</a>,
  abstract = {A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We find that while increasing the numbers of layers and parameters each have clear benefit, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and finds most of its benefit through the introduction of more weights. Our results (i) empirically confirm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.},
  annote = {$\backslash$begin\{itemize\}
  $\backslash$item Deeper models are preferred over shallow ones
  $\backslash$item Performance is independent of the number of units, when depth and parameters remains constant
  $\backslash$item Recurrent Neural Network:
    $\backslash$begin\{itemize\}
      $\backslash$item Convolutional architecture
      $\backslash$item all layers same number of feature maps
      $\backslash$item weights are tied across layers
      $\backslash$item ReLU in all layers
      $\backslash$item Max-pooling with non-overlaping windows
    $\backslash$end\{itemize\}
$\backslash$end\{itemize\}},
  archiveprefix = {arXiv},
  arxivid = {arXiv:1312.1847v1},
  author = {Eigen, David and Rolfe, Jason and Fergus, Rob and LeCun, Y},
  eprint = {arXiv:1312.1847v1},
  file = {:home/maikel/Documents/mendeley/1312.1847.pdf:pdf},
  journal = {arXiv preprint arXiv:1312.1847},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  pages = {1--9},
  title = {{Understanding Deep Architectures using a Recursive Convolutional Network}},
  url = {<a href="http://arxiv.org/abs/1312.1847 http://arxiv.org/pdf/1312.1847.pdf">http://arxiv.org/abs/1312.1847 http://arxiv.org/pdf/1312.1847.pdf</a>},
  year = {2013}
}
</pre>

<a name="Farabet2013"></a><pre>
@article{<a href="references.html#Farabet2013">Farabet2013</a>,
  abstract = {Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.},
  author = {Farabet, C and Couprie, Camille and Najman, Laurent and LeCun, Y},
  file = {:home/maikel/Documents/mendeley/farabet-pami-13.pdf:pdf},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  pages = {1915--1929},
  title = {{Learning hierarchical features for scene labeling}},
  url = {<a href="http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6338939 http://hal.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf">http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=6338939 http://hal.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf</a>},
  volume = {8},
  year = {2012}
}
</pre>

<a name="Erhan2010"></a><pre>
@article{<a href="references.html#Erhan2010">Erhan2010</a>,
  abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
  author = {Erhan, D and Bengio, Yoshua and Courville, Aaron},
  file = {:home/maikel/Documents/mendeley/Erhan, Bengio, Courville - 2010 - Why does unsupervised pre-training help deep learning.pdf:pdf},
  journal = {\ldots of Machine Learning \ldots},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  number = {2007},
  pages = {201--208},
  title = {{Why does unsupervised pre-training help deep learning?}},
  url = {<a href="http://dl.acm.org/citation.cfm?id=1756025 http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS2010\_ErhanCBV10.pdf">http://dl.acm.org/citation.cfm?id=1756025 http://machinelearning.wustl.edu/mlpapers/paper\_files/AISTATS2010\_ErhanCBV10.pdf</a>},
  volume = {9},
  year = {2010}
}
</pre>

<a name="LeCun1995"></a><pre>
@article{<a href="references.html#LeCun1995">LeCun1995</a>,
  author = {LeCun, Y and Bengio, Y},
  file = {:home/maikel/Documents/mendeley/LeCun, Bengio - 1995 - Convolutional networks for images, speech, and time series.pdf:pdf},
  journal = {\ldots handbook of brain theory and neural networks},
  keywords = {CNN,mscthesis},
  mendeley-tags = {CNN,mscthesis},
  pages = {1--14},
  title = {{Convolutional networks for images, speech, and time series}},
  url = {<a href="http://www.iro.umontreal.ca/labs/neuro/pointeurs/handbook-convo.pdf">http://www.iro.umontreal.ca/labs/neuro/pointeurs/handbook-convo.pdf</a>},
  year = {1995}
}
</pre>

<a name="LeCun1989a"></a><pre>
@article{<a href="references.html#LeCun1989a">LeCun1989a</a>,
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  author = {LeCun, Y and Boser, B and Denker, JS and Henderson, D and Howard, RE and Hubbard, W and Jackel, LD},
  file = {:home/maikel/Documents/mendeley/Unknown - Unknown - lecun-89e.pdf.pdf:pdf},
  journal = {Neural \ldots},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  title = {{Backpropagation applied to handwritten zip code recognition}},
  url = {<a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.1989.1.4.541">http://www.mitpressjournals.org/doi/abs/10.1162/neco.1989.1.4.541</a>},
  year = {1989}
}
</pre>

<a name="Hinton2006b"></a><pre>
@article{<a href="references.html#Hinton2006b">Hinton2006b</a>,
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  author = {Hinton, GE and Salakhutdinov, RR},
  file = {:home/maikel/Documents/mendeley/Hinton, Salakhutdinov - 2006 - Reducing the dimensionality of data with neural networks.pdf:pdf},
  journal = {Science},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  number = {July},
  pages = {504--507},
  title = {{Reducing the dimensionality of data with neural networks}},
  url = {<a href="http://www.sciencemag.org/content/313/5786/504.short http://www.lsv.uni-saarland.de/Seminar/ML\_for\_NLP\_SS12/HinSal06.pdf">http://www.sciencemag.org/content/313/5786/504.short http://www.lsv.uni-saarland.de/Seminar/ML\_for\_NLP\_SS12/HinSal06.pdf</a>},
  volume = {313},
  year = {2006}
}
</pre>

<a name="Hinton2007"></a><pre>
@article{<a href="references.html#Hinton2007">Hinton2007</a>,
  abstract = {The uniformity of the cortical architecture and the ability of functions to move to different areas of cortex following early damage strongly suggest that there is a single basic learning algorithm for extracting underlying structure from richly structured, high-dimensional sensory data. There have been many attempts to design such an algorithm, but until recently they all suffered from serious computational weaknesses. This chapter describes several of the proposed algorithms and shows how they can be combined to produce hybrid methods that work efficiently in networks with many layers and millions of adaptive connections.},
  author = {Hinton, Geoffrey},
  file = {:home/maikel/Documents/mendeley/Hinton - 2007 - To recognize shapes, first learn to generate images.pdf:pdf},
  journal = {Progress in brain research},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  title = {{To recognize shapes, first learn to generate images}},
  url = {<a href="http://www.sciencedirect.com/science/article/pii/S0079612306650346 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.9814\&rep=rep1\&type=pdf">http://www.sciencedirect.com/science/article/pii/S0079612306650346 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.9814\&rep=rep1\&type=pdf</a>},
  year = {2007}
}
</pre>

<a name="Hinton2006a"></a><pre>
@article{<a href="references.html#Hinton2006a">Hinton2006a</a>,
  abstract = {We show how to use “complementary priors” to eliminate the explaining-away effects thatmake inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of thewake-sleep algorithm. After fine-tuning, a networkwith three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to displaywhat the associativememory has in mind.},
  author = {Hinton, GE and Osindero, Simon and Teh, YW},
  file = {:home/maikel/Documents/mendeley/Hinton, Osindero, Teh - 2006 - A fast learning algorithm for deep belief nets.pdf:pdf},
  journal = {Neural computation},
  keywords = {mscthesis},
  mendeley-tags = {mscthesis},
  title = {{A fast learning algorithm for deep belief nets}},
  url = {<a href="http://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527 http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">http://www.mitpressjournals.org/doi/abs/10.1162/neco.2006.18.7.1527 http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf</a>},
  year = {2006}
}
</pre>

<hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.96.</em></p>
</body>
</html>
