<h3><a name="1962">1962</a></h3>
<ul><li>
<h4>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex 
<a href="#Hubel1962">[Hubel1962]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Hubel1962')"> [+] </a>
</h4>
<div id="abs_Hubel1962" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >None</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="1968">1968</a></h3>
<ul><li>
<h4>Receptive fields and functional architecture of monkey striate cortex 
<a href="#Hubel1968">[Hubel1968]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Hubel1968')"> [+] </a>
</h4>
<div id="abs_Hubel1968" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >1. The striate cortex was studied in lightly anaesthetized macaque and spider monkeys by recording extracellularly from single units and stimulating the retinas with spots or patterns of light. Most cells can be categorized as simple, complex, or hypercomplex, with response properties very similar to those previously described in the cat. On the average, however, receptive fields are smaller, and there is a greater sensitivity to changes in stimulus orientation. A small proportion of the cells are colour coded.2. Evidence is presented for at least two independent systems of columns extending vertically from surface to white matter. Columns of the first type contain cells with common receptive-field orientations. They are similar to the orientation columns described in the cat, but are probably smaller in cross-sectional area. In the second system cells are aggregated into columns according to eye preference. The ocular dominance columns are larger than the orientation columns, and the two sets of boundaries seem to be independent.3. There is a tendency for cells to be grouped according to symmetry of responses to movement; in some regions the cells respond equally well to the two opposite directions of movement of a line, but other regions contain a mixture of cells favouring one direction and cells favouring the other.4. A horizontal organization corresponding to the cortical layering can also be discerned. The upper layers (II and the upper two-thirds of III) contain complex and hypercomplex cells, but simple cells are virtually absent. The cells are mostly binocularly driven. Simple cells are found deep in layer III, and in IV A and IV B. In layer IV B they form a large proportion of the population, whereas complex cells are rare. In layers IV A and IV B one finds units lacking orientation specificity; it is not clear whether these are cell bodies or axons of geniculate cells. In layer IV most cells are driven by one eye only; this layer consists of a mosaic with cells of some regions responding to one eye only, those of other regions responding to the other eye. Layers V and VI contain mostly complex and hypercomplex cells, binocularly driven.5. The cortex is seen as a system organized vertically and horizontally in entirely different ways. In the vertical system (in which cells lying along a vertical line in the cortex have common features) stimulus dimensions such as retinal position, line orientation, ocular dominance, and perhaps directionality of movement, are mapped in sets of superimposed but independent mosaics. The horizontal system segregates cells in layers by hierarchical orders, the lowest orders (simple cells monocularly driven) located in and near layer IV, the higher orders in the upper and lower layers.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="1980">1980</a></h3>
<ul><li>
<h4>Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position 
<a href="#Fukushima1980">[Fukushima1980]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Fukushima1980')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Fukushima1980" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells', which show charac- teristics similar to simple cells or lower order hyper- complex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of self- organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cells of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern. 1.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 Reiteration of self-organized by ''learning without a teacher'' </li>
<li>
 Similar structure to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. </li>
<li>
 Network structure: <ul>
<li>
 Input layer (photoreceptor array) </li>
<li>
 Cascade of modules each one with : <ul>
<li>
 S-cells: in the first layer Simple cells or lower order hypercomplex cells </li>
<li>
 C-cells: in the second layer Complex cells or higher order hypercomplex cells </li>
</ul>
 </li>
</ul>
 </li>
<li>
 Hubel and Wiesel : the neural network in the visual cortex has a hierarchy structure: <ul>
<li>
 LGB (Lageral Geniculate Body) </li>
<li>
 Simple cells </li>
<li>
 Complex cells </li>
<li>
 Lower order hypercomplex cells </li>
<li>
 Higher order hypercomplex cells </li>
</ul>
 </li>
<li>
 a cell in a higher stage generally has tendency to respond selectively to a more complicated feature of the stimulus pattern </li>
<li>
 we extend the hierarchy model of Hubel and Wiesel, and \textbf{hypothesize} the existance of a similar hierarchy structure even in hte stages higher than hypercomplex cells. </li>
<li>
 In the last module, the receptive field of each C-cell becomes so large as to cover the whole area of input layer $U_0$, and each C-plane is so determined as to have only one C-cell </li>
<li>
 The output of an S-cell in the $k_l$-th S-plane in the l-th module is described below </li>
</ul>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="1989">1989</a></h3>
<ul><li>
<h4>Generalization and network design strategies 
<a href="#LeCun1989">[LeCun1989]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_LeCun1989')"> [+] </a>
</h4>
<div id="abs_LeCun1989" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >An interestmg property of connectiomst systems is their ability tolearn from examples. Although most recent work in the field concentrateson reducing learning times, the most important feature of a learning ma-chine is its generalization performance. It is usually accepted that goodgeneralization performance on real-world problems cannot be achievedunless some a pnon knowledge about the task is butlt Into the system.Back-propagation networks provide a way of specifymg such knowledgeby imposing constraints both on the architecture of the network and onits weights. In general, such constramts can be considered as particulartransformations of the parameter spaceBuilding a constramed network for image recogmtton appears to be afeasible task. We descnbe a small handwritten digit recogmtion problemand show that, even though the problem is linearly separable, single layernetworks exhibit poor generalizatton performance. Multtlayer constrainednetworks perform very well on this task when orgamzed in a hierarchicalstructure with shift invariant feature detectors.These results confirm the idea that minimizing the number of freeparameters in the network enhances generalization.</p>
</div>
</li><li>
<h4>Backpropagation applied to handwritten zip code recognition 
<a href="#LeCun1989a">[LeCun1989a]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_LeCun1989a')"> [+] </a>
</h4>
<div id="abs_LeCun1989a" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="1995">1995</a></h3>
<ul><li>
<h4>Convolutional networks for images, speech, and time series 
<a href="#LeCun1995">[LeCun1995]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_LeCun1995')"> [+] </a>
</h4>
<div id="abs_LeCun1995" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >None</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="1998">1998</a></h3>
<ul><li>
<h4>Gradient-based learning applied to document recognition 
<a href="#LeCun1998">[LeCun1998]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_LeCun1998')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_LeCun1998" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 LeNet-5 </li>
<li>
 Clarification: In this paper ``stride'' is not mentioned, but as Krizhevsky2012 et.al. started using it, new implementations of CNN need to define its value. </li>
<li>
 Conv: Convolutional layer </li>
<li>
 Subs: Subsampling layer (summed * coefficient + bias) </li>
<li>
 Full: Fully connected network </li>
<li>
 ERBF: Euclidian Radial Basis Function units <ul>
<li>
 input 32x32 pixel image (original images are 28x28) </li>
<li>
 Conv1 : <ul>
<li>
 6@28x28 filter 5x5 </li>
<li>
 stride 1 </li>
<li>
 Connections = $5*5*28*28*6 + 6*28*28 = 122,304$ </li>
<li>
 Train. param. = $5*5*6 + 6 = 156$ </li>
</ul>
 </li>
<li>
 Subs2 : <ul>
<li>
 6@14x14 range 2x2 </li>
<li>
 stride 2 </li>
<li>
 Connections = $6*28*28 + 6*14*14 = 5,880$ </li>
<li>
 Train. param. = coefficient + bias = $6 + 6 = 156$ </li>
</ul>
 </li>
<li>
 Conv3 : <ul>
<li>
 16@10x10 filter 5x5 </li>
<li>
 stride 1 </li>
<li>
 Connections $= 6*5*5*10*10*10 + 10*10*16 = 151,600$ </li>
<li>
 Train. param. $= 5*5*3*6 + 5*5*4*9 + 5*5*6*1 + 16 = 1,516$ </li>
<li>
 Note: </li>
<li>
 This layer is not completly connected, see table 1 for specific connections </li>
<li>
 Expected Connections $= 6*5*5*10*10*16 + 10*10*16 = 241,600$ </li>
<li>
 Expected train. param $= 5*5*16*6 + 16 = 2416$ </li>
</ul>
 </li>
<li>
 Subs4 : <ul>
<li>
 16@5x5 range 2x2 </li>
<li>
 stride 2 </li>
<li>
 Connections $= 16*10*10 + 16*5*5 = 2,000$ </li>
<li>
 Train. param. = coefficient + bias $= 16 + 16 = 32$ </li>
</ul>
 </li>
<li>
 Conv5 : <ul>
<li>
 120@1x1 filter 5x5 </li>
<li>
 stride 0 </li>
<li>
 Connections and train. param. $= 16*5*5*120 + 120 = 48,120$ </li>
</ul>
 </li>
<li>
 Full6 : 84 Atanh(Sa) <ul>
<li>
 Connections and train. param. $= 120*84 + 84 = 10,164$ </li>
</ul>
 </li>
<li>
 ERBF7 : 10 <ul>
<li>
 Connections and train. param. = $84*10 = 840$ </li>
</ul>
 </li>
</ul>
</li>
</ul>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2000">2000</a></h3>
<ul><li>
<h4>Independent component analysis applied to feature extraction from colour and stereo images. 
<a href="#Hoyer2000">[Hoyer2000]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Hoyer2000')"> [+] </a>
</h4>
<div id="abs_Hoyer2000" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Previous work has shown that independent component analysis (ICA) applied to feature extraction from natural image data yields features resembling Gabor functions and simple-cell receptive fields. This article considers the effects of including chromatic and stereo information. The inclusion of colour leads to features divided into separate red/green, blue/yellow, and bright/dark channels. Stereo image data, on the other hand, leads to binocular receptive fields which are tuned to various disparities. The similarities between these results and the observed properties of simple cells in the primary visual cortex are further evidence for the hypothesis that visual cortical neurons perform some type of redundancy reduction, which was one of the original motivations for ICA in the first place. In addition, ICA provides a principled method for feature extraction from colour and stereo images; such features could be used in image processing operations such as denoising and compression, as well as in pattern recognition.</p>
</div>
</li><li>
<h4>Emergence of phase-and shift-invariant features by decomposition of natural images into independent feature subspaces 
<a href="#Hyvarinen2000">[Hyvarinen2000]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Hyvarinen2000')"> [+] </a>
</h4>
<div id="abs_Hyvarinen2000" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Olshausen and Field (1996) applied the principle of independence maximization by sparse coding to extract features from natural images. This leads to the emergence of oriented linear filters that have simultaneous localization in space and in frequency, thus resembling Gabor functions and simple cell receptive fields. In this article, we show that the same principle of independence maximization can explain the emergence of phase- and shift-invariant features, similar to those found in complex cells. This new kind of emergence is obtained by maximizing the independence between norms of projections on linear subspaces (instead of the independence of simple linear filter outputs). The norms of the projections on such “independent feature subspaces” then indicate the values of invariant features.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2002">2002</a></h3>
<ul><li>
<h4>Why color management? 
<a href="#King2002">[King2002]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_King2002')"> [+] </a>
</h4>
<div id="abs_King2002" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >It seems that everywhere you look there is some article or discussion about color management. Why all the fuss? Do I need to management my colors? We have been creating colored artifacts for a very long time and I don't think we have needed color management. So why now? Most of these discussions also refer to the ICC. What is that? These and other questions will be answered in a straightforward manner in plain English. Adobe Systems has pioneered the use of desktop computers for color work, and the author has helped Adobe pick its way down conflicting color paths with confusing road signs over the last 10 years.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2003">2003</a></h3>
<ul><li>
<h4>Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis 
<a href="#Simard2003">[Simard2003]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Simard2003')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Simard2003" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Neural networks are a powerful technology forclassification of visual inputs arising from documents.However, there is a confusing plethora of different neuralnetwork methods that are used in the literature and inindustry. This paper describes a set of concrete bestpractices that document analysis researchers can use toget good results with neural networks. The mostimportant practice is getting a training set as large aspossible: we expand the training set by adding a newform of distorted data. The next most important practiceis that convolutional neural networks are better suited forvisual document tasks than fully connected networks. Wepropose that a simple “do-it-yourself” implementation ofconvolution with a flexible architecture is suitable formany visual document problems. This simpleconvolutional neural network does not require complexmethods, such as momentum, weight decay, structure-dependent learning rates, averaging layers, tangent prop,or even finely-tuning the architecture. The end result is avery simple yet general architecture which can yieldstate-of-the-art performance for document analysis. Weillustrate our claims on the MNIST set of English digitimages.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 Get a training set as large as possible </li>
<li>
 No need of complex methods, such as momentum, weight decay, structure-dependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture </li>
<li>
 Increment dataset by: <ul>
<li>
 Affine transformations: translations, scaling, homothety, similarity transformation, reflection, rotation, shear mapping, and compositions. </li>
<li>
 Elastic distortions </li>
</ul>
 </li>
<li>
 In this paper the authors justify the use of elastic deformations on MNIST data corresponding to uncontrolled oscillations of the hand muscles, dampened by inertia. </li>
<li>
 They get the best results on MNIST to date with CNN, affine and elastic transformations of the dataset (0.4\% error).</li>
</ul>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2006">2006</a></h3>
<ul><li>
<h4>A convolutional neural network approach for objective video quality assessment 
<a href="#Callet2006">[Callet2006]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Callet2006')"> [+] </a>
</h4>
<div id="abs_Callet2006" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >This paper describes an application of neural networks in the field of objective measurement method designed to automatically assess the perceived quality of digital videos. This challenging issue aims to emulate human judgment and to replace very complex and time consuming subjective quality assessment. Several metrics have been proposed in literature to tackle this issue. They are based on a general framework that combines different stages, each of them addressing complex problems. The ambition of this paper is not to present a global perfect quality metric but rather to focus on an original way to use neural networks in such a framework in the context of reduced reference (RR) quality metric. Especially, we point out the interest of such a tool for combining features and pooling them in order to compute quality scores. The proposed approach solves some problems inherent to objective metrics that should predict subjective quality score obtained using the single stimulus continuous quality evaluation (SSCQE) method. This latter has been adopted by video quality expert group (VQEG) in its recently finalized reduced referenced and no reference (RRNR-TV) test plan. The originality of such approach compared to previous attempts to use neural networks for quality assessment, relies on the use of a convolutional neural network (CNN) that allows a continuous time scoring of the video. Objective features are extracted on a frame-by-frame basis on both the reference and the distorted sequences; they are derived from a perceptual-based representation and integrated along the temporal axis using a time-delay neural network (TDNN). Experiments conducted on different MPEG-2 videos, with bit rates ranging 2-6 Mb/s, show the effectiveness of the proposed approach to get a plausible model of temporal pooling from the human vision system (HVS) point of view. More specifically, a linear correlation criteria, between objective and subjective scoring, up to 0.92 has been obtained on a- - set of typical TV videos</p>
</div>
</li><li>
<h4>A fast learning algorithm for deep belief nets 
<a href="#Hinton2006a">[Hinton2006a]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Hinton2006a')"> [+] </a>
</h4>
<div id="abs_Hinton2006a" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We show how to use “complementary priors” to eliminate the explaining-away effects thatmake inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of thewake-sleep algorithm. After fine-tuning, a networkwith three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to displaywhat the associativememory has in mind.</p>
</div>
</li><li>
<h4>Reducing the dimensionality of data with neural networks 
<a href="#Hinton2006b">[Hinton2006b]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Hinton2006b')"> [+] </a>
</h4>
<div id="abs_Hinton2006b" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2007">2007</a></h3>
<ul><li>
<h4>Robust object recognition with cortex-like mechanisms. 
<a href="#Serre2007">[Serre2007]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Serre2007')"> [+] </a>
</h4>
<div id="abs_Serre2007" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex.</p>
</div>
</li><li>
<h4>To recognize shapes, first learn to generate images 
<a href="#Hinton2007">[Hinton2007]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Hinton2007')"> [+] </a>
</h4>
<div id="abs_Hinton2007" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >The uniformity of the cortical architecture and the ability of functions to move to different areas of cortex following early damage strongly suggest that there is a single basic learning algorithm for extracting underlying structure from richly structured, high-dimensional sensory data. There have been many attempts to design such an algorithm, but until recently they all suffered from serious computational weaknesses. This chapter describes several of the proposed algorithms and shows how they can be combined to produce hybrid methods that work efficiently in networks with many layers and millions of adaptive connections.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2008">2008</a></h3>
<ul><li>
<h4>Deep learning via semi-supervised embedding 
<a href="#Weston2008">[Weston2008]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Weston2008')"> [+] </a>
</h4>
<div id="abs_Weston2008" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We show how nonlinear semi-supervised embedding algorithms popular for use with “shallow” learning techniques such as kernel methods can be easily applied to deep multi-layer architectures, either as a regularizer at the output layer, or on each layer of the architecture. Compared to standard supervised backpropagation this can give significant gains. This trick provides a simple alternative to existing approaches to semi-supervised deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2009">2009</a></h3>
<ul><li>
<h4>Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations 
<a href="#Lee2009">[Lee2009]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Lee2009')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Lee2009" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 Probabilistic max-pooling </li>
<li>
 Scale DBN to real-sized images <ul>
<li>
 Computationally intractable </li>
<li>
 Need invariance in representation </li>
</ul>
 </li>
<li>
 RBM <ul>
<li>
 Binary valued: Independent Bernoulli random variables </li>
<li>
 Real valued: Gaussian with diagonal covariance </li>
<li>
 Training: <ul>
<li>
 Stochastic gradient ascent on log-likelihood of training data </li>
<li>
 Contrastive divergence approximation </li>
</ul>
 </li>
</ul>
 </li>
<li>
 Convolutional RBM <ul>
<li>
 detection layers: convolving feature maps </li>
<li>
 pooling layers: shrink the representation <ul>
<li>
 Block: CxC from bottom layer </li>
<li>
 Max-pooling : minimizes energy subject to only one unit can be active. </li>
</ul>
 </li>
<li>
 Sparsity regularization: hidden units have a mean activation close to a small constant </li>
</ul>
 </li>
<li>
 Convolutional Deep belief network <ul>
<li>
 Stacking CRBM on top of one another </li>
<li>
 Training: <ul>
<li>
 Gibbs sampling </li>
<li>
 Mean-field (5 iterations in this paper) </li>
</ul>
 </li>
</ul>
</li>
</ul>
</div>
</li><li>
<h4>Learning Deep Architectures for AI 
<a href="#Bengio2009">[Bengio2009]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Bengio2009')"> [+] </a>
</h4>
<div id="abs_Bengio2009" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.</p>
</div>
</li><li>
<h4>Stacks of convolutional restricted Boltzmann machines for shift-invariant feature learning 
<a href="#Norouzi2009">[Norouzi2009]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Norouzi2009')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Norouzi2009" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >In this paper we present a method for learning class-specific features for recognition. Recently a greedy layer-wise procedure was proposed to initialize weights of deep belief networks, by viewing each layer as a separate restricted Boltzmann machine (RBM). We develop the convolutional RBM (C-RBM), a variant of the RBM model in which weights are shared to respect the spatial structure of images. This framework learns a set of features that can generate the images of a specific object class. Our feature extraction model is a four layer hierarchy of alternating filtering and maximum subsampling. We learn feature parameters of the first and third layers viewing them as separate C-RBMs. The outputs of our feature extraction hierarchy are then fed as input to a discriminative classifier. It is experimentally demonstrated that the extracted features are effective for object detection, using them to obtain performance comparable to the state of the art on handwritten digit recognition and pedestrian detection.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 New Convolutional Restricted Boltzmann Machine (C-RBM) </li>
<li>
 Comparable state-of-the-art on handwritten digit recognition and pedestrian detection </li>
<li>
 RBM <ul>
<li>
 Probabilistic model </li>
<li>
 hidden variables independent given observerd data </li>
<li>
 Not capture explicitly spacial structure of images </li>
</ul>
 </li>
<li>
 C-RBM <ul>
<li>
 Include spatial locality and weight sharing </li>
<li>
 Favors filters with high response on training images </li>
<li>
 Unsupervised learning using Contrastive Divergence </li>
<li>
 Layerwise training for stacks of RBMs </li>
<li>
 Convolutional connections are employed in a generative Markov Random Field architecture </li>
<li>
 Hidden units divided into K feature maps </li>
<li>
 Convolution problems <ul>
<li>
 Boundary units are withinb a smaller number of subwindows compared to the interior pixels </li>
<li>
 middle pixels may contribute to $K_{xy}$ features </li>
<li>
 Separation of boundary variables ($v^b$) from middle variables ($v^m$) </li>
<li>
 Problems sampling from boundary pixels (not have nough features) </li>
<li>
 Over completeness because of K-features </li>
<li>
 Sampling creates images very similar to the original ones </li>
<li>
 Need of more Gibbs sampling steps </li>
<li>
 Their solution is to fix hidden bias terms $c$ during training </li>
</ul>
 </li>
</ul>
 </li>
<li>
 Multilayer C-RBMs <ul>
<li>
 Subsampling takes maximum conditional feature probability over non-overlapping subwindows of feature maps </li>
<li>
 Architecture <ul>
<li>
 discriminative layer (SVM) </li>
<li>
 max pooling </li>
<li>
 convolution </li>
<li>
 max pooling </li>
<li>
 convolution </li>
<li>
 input </li>
</ul>
 </li>
<li>
 On pedestrians also HOG is used in discriminative layer </li>
</ul>
 </li>
<li>
 MNIST dataset <ul>
<li>
 Discriminative layer with RBF kernel </li>
<li>
 10 one-vs-rest binary SVMs </li>
<li>
 1st layer 15 feature maps </li>
<li>
 2nd layer 2x2 non-overlapping subwindos </li>
<li>
 3rd layer 15 feature maps </li>
<li>
 4th layer </li>
</ul>
 </li>
<li>
 Comparison with Large CNN <ul>
<li>
 C-RBM is better when training is small </li>
</ul>
 </li>
<li>
 Pedestrian dataset <ul>
<li>
 1st layer 7x7 15 feature maps </li>
<li>
 2nd layer 4x4 subsampling </li>
<li>
 3rd layer 15x5x5 30 feature maps </li>
<li>
 4th layer 2x2 subsampling </li>
<li>
 + HOG </li>
<li>
 Discriminative layer with linear kernel </li>
</ul>
</li>
</ul>
</div>
</li><li>
<h4>Evaluation of local spatio-temporal features for action recognition 
<a href="#Wang2009">[Wang2009]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Wang2009')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Wang2009" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Local space-time features have recently become a popular video representation for action recognition. Several methods for feature localization and description have been proposed in the literature and promising recognition results were demonstrated for a number of action classes. The comparison of existing methods, however, is often limited given the different experimental settings used. The purpose of this paper is to evaluate and compare previously proposed space-time features in a common experimental setup. In particular, we consider four different feature detectors and six local feature descriptors and use a standard bag-of-features SVM approach for action recognition. We investigate the performance of these methods on a total of 25 action classes distributed over three datasets with varying difficulty. Among interesting conclusions, we demonstrate that regular sampling of space-time features consistently outperforms all tested space-time interest point detectors for human actions in realistic settings. We also demonstrate a consistent ranking for the majority of methods over different datasets and discuss their advantages and limitations.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 Detectors <ul>
<li>
 Harris3D </li>
<li>
 Cuboid </li>
<li>
 Hessian </li>
<li>
 Dense sampling </li>
</ul>
 </li>
<li>
 Descriptors <ul>
<li>
 HOG/HOF </li>
<li>
 HOG3D </li>
<li>
 ESURF (extended SURF) </li>
</ul>
 </li>
<li>
 Datasets <ul>
<li>
 KTH actions <ul>
<li>
 6 human action classes </li>
<li>
 walking, jogging, running, boxing, waving and clapping </li>
<li>
 25 subjects </li>
<li>
 4 scenarios </li>
<li>
 2391 video samples </li>
<li>
 \url{http://www.nada.kth.se/cvap/actions/} </li>
</ul>
 </li>
<li>
 UCF sport actions <ul>
<li>
 10 human action classes </li>
<li>
 winging, diving, kicking, weight-lifting, horse-riding, running, skateboarding, swinging, golf swinging and walking </li>
<li>
 150 video samples </li>
<li>
 \url{http://crcv.ucf.edu/data/UCF_Sports_Action.php} </li>
</ul>
 </li>
<li>
 Hollywood2 actions <ul>
<li>
 12 action classes </li>
<li>
 answering the hone, driving car, eating, fighting, geting out of the car, hand shaking, hugging, kissing, running, sitting down, sitting up, and standing up. </li>
<li>
 69 Hollywood movies </li>
<li>
 1707 video samples </li>
<li>
 \url{http://www.di.ens.fr/~laptev/actions/hollywood2/} </li>
</ul>
 </li>
</ul>
</li>
</ul>
</div>
</li><li>
<h4>Actions in context 
<a href="#Marszalek2009">[Marszalek2009]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Marszalek2009')"> [+] </a>
</h4>
<div id="abs_Marszalek2009" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >This paper exploits the context of natural dynamic scenes for human action recognition in video. Human actions are frequently constrained by the purpose and the physical properties of scenes and demonstrate high correlation with particular scene classes. For example, eating often happens in a kitchen while running is more common outdoors. The contribution of this paper is three-fold: (a) we automatically discover relevant scene classes and their correlation with human actions, (b) we show how to learn selected scene classes from video without manual supervision and (c) we develop a joint framework for action and scene recognition and demonstrate improved recognition of both in natural video. We use movie scripts as a means of automatic supervision for training. For selected action classes we identify correlated scene classes in text and then retrieve video samples of actions and scenes for training using script-to-video alignment. Our visual models for scenes and actions are formulated within the bag-of-features framework and are combined in a joint scene-action SVM-based classifier. We report experimental results and validate the method on a new large dataset with twelve action classes and ten scene classes acquired from 69 movies.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2010">2010</a></h3>
<ul><li>
<h4>Learning Convolutional Feature Hierarchies for Visual Recognition 
<a href="#Kavukcuoglu2010">[Kavukcuoglu2010]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Kavukcuoglu2010')"> [+] </a>
</h4>
<div id="abs_Kavukcuoglu2010" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We propose an unsupervised method for learning multi-stage hierarchies of sparseconvolutional features. While sparse coding has become an increasingly popularmethod for learning visual features, it is most often trained at the patch level.Applying the resulting filters convolutionally results in highly redundant codesbecause overlapping patches are encoded in isolation. By training convolutionallyover large image windows, our method reduces the redudancy between featurevectors at neighboring locations and improves the efficiency of the overall repre-sentation. In addition to a linear decoder that reconstructs the image from sparsefeatures, our method trains an efficient feed-forward encoder that predicts quasi-sparse features from the input. While patch-based training rarely produces any-thing but oriented edge detectors, we show that convolutional training produceshighly diverse filters, including center-surround filters, corner detectors, cross de-tectors, and oriented grating detectors. We show that using these filters in multi-stage convolutional network architecture improves performance on a number ofvisual recognition and detection tasks</p>
</div>
</li><li>
<h4>Tiled convolutional neural networks 
<a href="#Ngiam2010">[Ngiam2010]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Ngiam2010')"> [+] </a>
</h4>
<div id="abs_Ngiam2010" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Convolutional neural networks (CNNs) have been successfully applied to manytasks such as digit and object recognition. Using convolutional (tied) weightssignificantly reduces the number of parameters that have to be learned, and alsoallows translational invariance to be hard-coded into the architecture. In this pa-per, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolution neural networks (Tiled CNNs), which usea regular “tiled” pattern of tied weights that does not require that adjacent hiddenunits share identical weights, but instead requires only that hidden units k stepsaway from each other to have tied weights. By pooling over neighboring units,this architecture is able to learn complex invariances (such as scale and rotationalinvariance) beyond translational invariance. Further, it also enjoys much of CNNs’advantage of having a relatively small number of learned parameters (such as easeof learning and greater scalability). We provide an efficient learning algorithm forTiled CNNs based on Topographic ICA, and show that learning complex invariantfeatures allows us to achieve highly competitive results for both the NORB andCIFAR-10 datasets.</p>
</div>
</li><li>
<h4>Convolutional learning of spatio-temporal features 
<a href="#Taylor2010">[Taylor2010]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Taylor2010')"> [+] </a>
</h4>
<div id="abs_Taylor2010" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We address the problem of learning good features for understanding video data. We introduce a model that learns latent representations of image sequences from pairs of successive images. The convolutional architecture of our model allows it to scale to realistic image sizes whilst using a compact parametrization. In experiments on the NORB dataset, we show our model extracts latent “flow fields” which correspond to the transformation between the pair of input frames. We also use our model to extract low-level motion features in a multi-stage architecture for action recognition, demonstrating competitive performance on both the KTH and Hollywood2 datasets.</p>
</div>
</li><li>
<h4>Why does unsupervised pre-training help deep learning? 
<a href="#Erhan2010">[Erhan2010]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Erhan2010')"> [+] </a>
</h4>
<div id="abs_Erhan2010" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.</p>
</div>
</li><li>
<h4>Convolutional Deep Belief Networks on CIFAR-10 
<a href="#Krizhevsky2010">[Krizhevsky2010]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Krizhevsky2010')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Krizhevsky2010" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We describe how to train a two-layer convolutional Deep Belief Network (DBN) on the 1.6 million tiny imagesdataset.When training a convolutional DBN, one must decide what to do with the edge pixels of teh images. Asthe pixels near the edge of an image contribute to the fewest convolutional filter outputs, the model maysee it fit to tailor its few convolutional filters to better model the edge pixels. This is undesirable becaue itusually comes at the expense of a good model for the interior parts of the image. We investigate several waysof dealing with the edge pixels when training a convolutional DBN. Using a combination of locally-connectedconvolutional units and globally-connected units, as well as a few tricks to reduce the effects of overfitting,we achieve state-of-the-art performance in the classification task of the CIFAR-10 subset of the tiny imagesdataset.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 Detectors <ul>
<li>
 Harris3D </li>
<li>
 Cuboid </li>
<li>
 Hessian </li>
<li>
 Dense sampling </li>
</ul>
 </li>
<li>
 Descriptors <ul>
<li>
 HOG/HOF </li>
<li>
 HOG3D </li>
<li>
 ESURF (extended SURF) </li>
</ul>
 </li>
<li>
 Datasets <ul>
<li>
 KTH actions <ul>
<li>
 6 human action classes </li>
<li>
 walking, jogging, running, boxing, waving and clapping </li>
<li>
 25 subjects </li>
<li>
 4 scenarios </li>
<li>
 2391 video samples </li>
<li>
 $http://www.nada.kth.se/cvap/actions/$ </li>
</ul>
 </li>
<li>
 UCF sport actions <ul>
<li>
 10 human action classes </li>
<li>
 winging, diving, kicking, weight-lifting, horse-riding, running, skateboarding, swinging, golf swinging and walking </li>
<li>
 150 video samples </li>
<li>
 $http://crcv.ucf.edu/data/UCF_Sports_Action.php$ </li>
</ul>
 </li>
<li>
 Hollywood2 actions <ul>
<li>
 12 action classes </li>
<li>
 answering the hone, driving car, eating, fighting, geting out of the car, hand shaking, hugging, kissing, running, sitting down, sitting up, and standing up. </li>
<li>
 69 Hollywood movies </li>
<li>
 1707 video samples </li>
<li>
 $http://www.di.ens.fr/~laptev/actions/hollywood2/$ </li>
</ul>
 </li>
</ul>
</li>
</ul>
</div>
</li><li>
<h4>Tiled convolutional neural networks. 
<a href="#Le2010">[Le2010]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Le2010')"> [+] </a>
</h4>
<div id="abs_Le2010" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights significantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hardcoding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled ” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs’ advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2011">2011</a></h3>
<ul><li>
<h4>Building high-level features using large scale unsupervised learning 
<a href="#Le2011">[Le2011]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Le2011')"> [+] </a>
</h4>
<div id="abs_Le2011" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200×200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.</p>
</div>
</li><li>
<h4>Stacked convolutional auto-encoders for hierarchical feature extraction 
<a href="#Masci2011">[Masci2011]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Masci2011')"> [+] </a>
</h4>
<div id="abs_Masci2011" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We present a novel convolutional auto-encoder (CAE) for unsupervised feature learning. A stack of CAEs forms a convolutional neural network (CNN). Each CAE is trained using conventional on-line gradient descent without additional regularization terms. A max-pooling layer is essential to learn biologically plausible features consistent with those found by previous approaches. Initializing a CNN with filters of a trained CAE stack yields superior performance on a digit (MNIST) and an object recognition (CIFAR10) benchmark.</p>
</div>
</li><li>
<h4>Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis 
<a href="#Le2011a">[Le2011a]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Le2011a')"> [+] </a>
</h4>
<div id="abs_Le2011a" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3% and 75.8% respectively, which are approximately 5% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/~wzou/.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2012">2012</a></h3>
<ul><li>
<h4>Learning hierarchical features for scene labeling 
<a href="#Farabet2013">[Farabet2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Farabet2013')"> [+] </a>
</h4>
<div id="abs_Farabet2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.</p>
</div>
</li><li>
<h4>Gated boltzmann machine in texture modeling 
<a href="#Hao2012">[Hao2012]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Hao2012')"> [+] </a>
</h4>
<div id="abs_Hao2012" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >In this paper, we consider the problem of modeling complex texture information using undirected probabilistic graphical models. Texture is a special type of data that one can better understand by considering its local structure. For that purpose, we propose a convolutional variant of the Gaussian gated Boltzmann machine (GGBM) [12], inspired by the co-occurrence matrix in traditional texture analysis. We also link the proposed model to a much simpler Gaussian restricted Boltzmann machine where convolutional features are computed as a preprocessing step. The usefulness of the model is illustrated in texture classification and reconstruction experiments.</p>
</div>
</li><li>
<h4>ImageNet Classification with Deep Convolutional Neural Networks 
<a href="#Krizhevsky2012">[Krizhevsky2012]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Krizhevsky2012')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Krizhevsky2012" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We trained a large, deep convolutional neural network to classify the 1.2 millionhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%and 17.0% which is considerably better than the previous state-of-the-art. Theneural network, which has 60 million parameters and 650,000 neurons, consistsof five convolutional layers, some of which are followed by max-pooling layers,and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connectedlayers we employed a recently-developed regularization method called “dropout”that proved to be very effective. We also entered a variant of this model in theILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,compared to 26.2% achieved by the second-best entry.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 CNN architecture: <ul>
<li>
 650.000 neurons (60 million parameters) </li>
<li>
 5 convolutional layers </li>
<li>
 Some of them followed by a max-pooling layer </li>
<li>
 3 fully-connected layers </li>
<li>
 1 1000-way softmax </li>
</ul>
 </li>
<li>
 Dropout regularization method to reduce overfitting in 3 fully-connected layers </li>
<li>
 Training time: 5-6 days on two GTX 580 3GB GPUs </li>
<li>
 Dataset: <ul>
<li>
 ILSVRC-2010 </li>
<li>
 Down-sampled images to a fixed resolution of 256x256 </li>
<li>
 Substract the mean activity ofver training set from each pixel </li>
</ul>
 </li>
<li>
 ReLU: <ul>
<li>
 $f(x) = \max(0,x)$ </li>
<li>
 Faster than tanh </li>
<li>
 ReLU: 6 epochs </li>
<li>
 tanh: 36 more epochs to achieve same performance </li>
</ul>
 </li>
<li>
 Local Response Normalization <ul>
<li>
 $1.2$ and $1.4\%$ error reduction </li>
<li>
 Helps generalization </li>
<li>
 $b_{x,y}^i = a_{x,y}^i / \left( k + \alpha \sum\limits_{j=\max(0,i-n/2)}^{\min(N-1,i+n/2)} (a_{x,y}^j)^2 \right)^\beta$ </li>
<li>
 $k=2, n=5, \alpha=10^-4$, and $\beta=0.75$ </li>
</ul>
 </li>
<li>
 Overlapping Pooling <ul>
<li>
 $0.3$ and $0.4\%$ error reduction </li>
<li>
 grid $3 \prod 3$ </li>
<li>
 stride = 2 </li>
<li>
 Overlap each pooling one column pixel </li>
</ul>
 </li>
<li>
 Overall Architecture <ul>
<li>
 224x224x3 (RGB image) </li>
<li>
 Conv 96 kernels of size 11x11x3 with stride of 4 pixels </li>
<li>
 Response-Normalized and max-pooling </li>
<li>
 Conv 256 kernels of size 5x5x48 with stride of ? pixels </li>
<li>
 Response-Normalized and max-pooling </li>
<li>
 Conv 384 kernels of size 3x3x256 </li>
<li>
 Conv 384 kernels of size 3x3x192 </li>
<li>
 Conv 256 kernels of size 3x3x192 </li>
<li>
 ¿Response-Normalized? and Max-pooling </li>
<li>
 Fully connected 4096 </li>
<li>
 Fully connected 4096 </li>
<li>
 Fully connected 1000 </li>
<li>
 Softmax </li>
</ul>
\figuremacro{figures/krizhevsky2012_convnet.pdf}{Architecture of the CNN}{} </li>
<li>
 Data augmentation <ul>
<li>
 $0.1$ error reduction </li>
<li>
 Original images escaled scaled and croped to 256x256 </li>
<li>
 Extract 5 images of 224x224 from corners plus center </li>
<li>
 Mirror horizontally and get 5 more images </li>
<li>
 Augment data altering RGB channels: <ul>
<li>
 Perform PCA on RGB throughout the training set </li>
<li>
 Each training image add multiples of PCs with gaussian noise </li>
</ul>
 </li>
</ul>
 </li>
<li>
 Dropout <ul>
<li>
 Put to zero the output of neurons with probability 0.5 </li>
<li>
 At test time multiply the outputs by 0.5 </li>
<li>
 Two first fully-connected layers </li>
<li>
 Solves overfitting </li>
<li>
 Dobules the number of iterations required to ocnverge </li>
</ul>
 </li>
<li>
 Details of learning <ul>
<li>
 batch size = 128 </li>
<li>
 momentum 0.9 </li>
<li>
 weight decay 0.0005 </li>
<li>
 Initial weights from zero-mean Gaussian std=0.01 </li>
<li>
 biases = 1 on second, fourth, fifth Conv and fully-connected </li>
<li>
 biases = 0 on the rest </li>
</ul>
 </li>
<li>
 Evaluation <ul>
<li>
 Consider the feature activations induced by an image at the last, 4096-dimensional hidden layer </li>
</ul>
</li>
</ul>
</div>
</li><li>
<h4>The Stanford / Technicolor / Fraunhofer HHI Video 
<a href="#Araujo">[Araujo]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Araujo')"> [+] </a>
</h4>
<div id="abs_Araujo" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Video search has become a very important tool, with the ever-growing size of multimedia collections. This work introduces our Video Semantic Indexing system. Our experiments show that Residual Vectors provide an efficient way of aggregat- ing local descriptors, with complementary gain with respect to BoVW. Also, we show that systems using a limited number of descriptors and machine learning techniques can still be quite effective. Our first participation at the TRECVID evaluation has been very fruitful: our team was ranked 6th in the light version of the Semantic Indexing task.</p>
</div>
</li><li>
<h4>Improving neural networks by preventing co-adaptation of feature detectors 
<a href="#Hinton2012">[Hinton2012]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Hinton2012')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Hinton2012" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 Paper about Dropout </li>
<li>
 Standard way to reduce test error <ul>
<li>
 averaging different models </li>
<li>
 Computationally expensive in training and test </li>
</ul>
 </li>
<li>
 Dropout <ul>
<li>
 Small training set </li>
<li>
 Prevents ``overfitting'' </li>
<li>
 They use $50\%$ </li>
<li>
 Instead of L2 norm, they set an upper bound for each individual neuron. </li>
<li>
 Mean network : At test time divide all the outgoing weights by 2 to compensate dropout </li>
<li>
 Specific case <ul>
<li>
 Single hidden layer network </li>
<li>
 N hidden units </li>
<li>
 ``Softmax'' output </li>
<li>
 $50\%$ dropout </li>
<li>
 during test using mean network </li>
<li>
 Exactly equivalent to taking the geometric mean of the probability distributions over labels predicted by all $2^N$ possible networks </li>
</ul>
 </li>
</ul>
 </li>
<li>
 Results <ul>
<li>
 MNIST <ul>
<li>
 No dropout : 160 errors </li>
<li>
 Dropbout : 130 errors </li>
<li>
 Dropout + rm random $20\%$ pixels : 110 errors </li>
<li>
 Deep Boltzmann machine : 88 errors </li>
<li>
 + Dropout : 79 errors \figuremacro{figures/Hinton2012_fig5}{Visualization of features learned by first layer hidden units}{left without dropout and right using dropout} </li>
</ul>
 </li>
<li>
 TIMIT <ul>
<li>
 4 Fully-connected hidden layers 4000 units per layer </li>
<li>
 + 185 ``softmax'' output units </li>
<li>
 Without dropout : $22.7\%$ </li>
<li>
 Dropout on hidden units : $19.7\%$ </li>
</ul>
 </li>
<li>
 CIFAR-10 <ul>
<li>
 Best published : $18.5\%$ </li>
<li>
 3 Conv+Max-pool 1 Fully : $16.6\%$ </li>
<li>
 + Dropout in last hidden layer : $15.6\%$ </li>
</ul>
 </li>
<li>
 ImageNet <ul>
<li>
 Average of 6 separate models : $47.2\%$ </li>
<li>
 state-of-the-art : $45.7\%$ </li>
<li>
 5 Conv+Max-pool </li>
<li>
 + 2 Fully </li>
<li>
 + 1000 ``softmax'' </li>
<li>
 Without dropout : $48.6\%$ </li>
<li>
 Dropout in the 6th : $42.4\%$ </li>
</ul>
 </li>
<li>
 Reuters <ul>
<li>
 2 fully of 2000 hidden units </li>
<li>
 Without dropout : $31.05\%$ </li>
<li>
 Dropout : $29.62\%$ </li>
</ul>
 </li>
</ul>
</li>
</ul>
</div>
</li><li>
<h4>Recognizing 50 human action categories of web videos 
<a href="#Reddy2012">[Reddy2012]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Reddy2012')"> [+] </a>
</h4>
<div id="abs_Reddy2012" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Action recognition on large categories of unconstrained videos taken from the web is a very challenging problem compared to datasets like KTH (6 actions), IXMAS (13 actions), and Weizmann (10 actions). Challenges like camera motion, different viewpoints, large interclass variations, cluttered background, occlusions, bad illumination conditions, and poor quality of web videos cause the majority of the state-of-the-art action recognition approaches to fail. Also, an increased number of categories and the inclusion of actions with high confusion add to the challenges. In this paper, we propose using the scene context information obtained from moving and stationary pixels in the key frames, in conjunction with motion features, to solve the action recognition problem on a large (50 actions) dataset with videos from the web. We perform a combination of early and late fusion on multiple features to handle the very large number of categories. We demonstrate that scene context is a very important feature to perform action recognition on very large datasets. The proposed method does not require any kind of video stabilization, person detection, or tracking and pruning of features. Our approach gives good performance on a large number of action categories; it has been tested on the UCF50 dataset with 50 action categories, which is an extension of the UCF YouTube Action (UCF11) dataset containing 11 action categories. We also tested our approach on the KTH and HMDB51 datasets for comparison.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2013">2013</a></h3>
<ul><li>
<h4>Mitosis detection in breast cancer histology images with deep neural networks 
<a href="#Ciresan2013">[Ciresan2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Ciresan2013')"> [+] </a>
</h4>
<div id="abs_Ciresan2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We use deep max-pooling convolutional neural networks to detect mitosis in breast histology images. The networks are trained to classify each pixel in the images, using as context a patch centered on the pixel. Simple postprocessing is then applied to the network output. Our approach won the ICPR 2012 mitosis detection competition, outperforming other contestants by a significant margin.</p>
</div>
</li><li>
<h4>Understanding Deep Architectures using a Recursive Convolutional Network 
<a href="#Eigen2013">[Eigen2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Eigen2013')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Eigen2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >A key challenge in designing convolutional network models is sizing them appropriately. Many factors are involved in these decisions, including number of layers, feature maps, kernel sizes, etc. Complicating this further is the fact that each of these influence not only the numbers and dimensions of the activation units, but also the total number of parameters. In this paper we focus on assessing the independent contributions of three of these linked variables: The numbers of layers, feature maps, and parameters. To accomplish this, we employ a recursive convolutional network whose weights are tied between layers; this allows us to vary each of the three factors in a controlled setting. We find that while increasing the numbers of layers and parameters each have clear benefit, the number of feature maps (and hence dimensionality of the representation) appears ancillary, and finds most of its benefit through the introduction of more weights. Our results (i) empirically confirm the notion that adding layers alone increases computational power, within the context of convolutional layers, and (ii) suggest that precise sizing of convolutional feature map dimensions is itself of little concern; more attention should be paid to the number of parameters in these layers instead.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 Deeper models are preferred over shallow ones </li>
<li>
 Performance is independent of the number of units, when depth and parameters remains constant </li>
<li>
 Recurrent Neural Network: <ul>
<li>
 Convolutional architecture </li>
<li>
 all layers same number of feature maps </li>
<li>
 weights are tied across layers </li>
<li>
 ReLU in all layers </li>
<li>
 Max-pooling with non-overlaping windows </li>
</ul>
</li>
</ul>
</div>
</li><li>
<h4>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps 
<a href="#Simonyan2013">[Simonyan2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Simonyan2013')"> [+] </a>
</h4>
<div id="abs_Simonyan2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].</p>
</div>
</li><li>
<h4>Visualizing and Understanding Convolutional Networks 
<a href="#Zeiler2013">[Zeiler2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Zeiler2013')"> [+] </a>
</h4>
<div id="abs_Zeiler2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky et.al. on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.</p>
</div>
</li><li>
<h4>Action and event recognition with Fisher vectors on a compact feature set 
<a href="#Oneata2013">[Oneata2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Oneata2013')"> [+] </a>
</h4>
<div id="abs_Oneata2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Action recognition in uncontrolled video is an important and challenging computer vision problem. Recent progress in this area is due to new local features and models that capture spatio-temporal structure between local features, or human-object interactions. Instead of working towards more complex models, we focus on the low-level features and their encoding. We evaluate the use of Fisher vectors as an alternative to bag-of-word histograms to aggregate a small set of state-of-the-art low-level descriptors, in combination with linear classifiers. We present a large and varied set of evaluations, considering (i) classification of short actions in five datasets, (ii) localization of such actions in feature-length movies, and (iii) large-scale recognition of complex events. We find that for basic action recognition and localization MBH features alone are enough for state-of-the-art performance. For complex events we find that SIFT and MFCC features provide complementary cues. On all three problems we obtain state-of-the-art results, while using fewer features and less complex models.</p>
</div>
</li><li>
<h4>TRECVID 2013 – An Introduction to the Goals , Tasks , Data , Evaluation Mechanisms , and Metrics 
<a href="#Over2013">[Over2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Over2013')"> [+] </a>
</h4>
<div id="abs_Over2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >None</p>
</div>
</li><li>
<h4>Quaero at TRECVid 2013 : Semantic Indexing 
<a href="#Safadi2013">[Safadi2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Safadi2013')"> [+] </a>
</h4>
<div id="abs_Safadi2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >The Quaero group is a consortium of French and German organizations working on Multimedia Indexing and Retrieval. LIG, INRIA and KIT participated to the semantic indexing task and LIG participated to the organization of this task. This paper describes these participations. For the semantic indexing task, our approach uses a six-stages processing pipelines for computing scores for the likelihood of a video shot to contain a target concept. These scores are then used for producing a ranked list of images or shots that are the most likely to contain the target concept. The pipeline is composed of the following steps: descriptor extraction, descriptor optimization, classi cation, fusion of descriptor variants, higher-level fusion, and re-ranking. We used a number of di erent descriptors and a hierarchical fusion strategy. We also used conceptual feedback by adding a vector of classi cation score to the pool of descriptors. The best Quaero run has a Mean Inferred Average Precision of 0.2692, which ranked us 3rd out of 16 participants. We also organized the TRECVid SIN 2012 collaborative annotation.</p>
</div>
</li><li>
<h4>MediaMill at TRECVID 2013: Searching Concepts, Objects, Instances and Events in Video 
<a href="#Snoek2013">[Snoek2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Snoek2013')"> [+] </a>
</h4>
<div id="abs_Snoek2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >In this paper we summarize our TRECVID 2013 video retrieval experiments. The MediaMill team participated in four tasks: concept detection, object localization, in- stance search, and event recognition. For all tasks the starting point is our top-performing bag-of-words system of TRECVID 2008-2012, which uses color SIFT descrip- tors, average and difference coded into codebooks with spa- tial pyramids and kernel-based machine learning. New this year are concept detection with deep learning, concept detec- tion without annotations, object localization using selective search, instance search by reranking, and event recognition based on concept vocabularies. Our experiments focus on es- tablishing the video retrieval value of the innovations. The 2013 edition of the TRECVID benchmark has again been a fruitful participation for the MediaMill team, resulting in the best result for concept detection, concept detection with- out annotation, object localization, concept pair detection, and visual event recognition with few examples.</p>
</div>
</li><li>
<h4>Semi-supervised Learning of Feature Hierarchies for Object Detection in a Video 
<a href="#Yang2013">[Yang2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Yang2013')"> [+] </a>
</h4>
<div id="abs_Yang2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We propose a novel approach to boost the performance of generic object detectors on videos by learning video-specific features using a deep neural network. The insight behind our proposed approach is that an object appearing in different frames of a video clip should share similar features, which can be learned to build better detectors. Unlike many supervised detector adaptation or detection-by-tracking methods, our method does not require any extra annotations or utilize temporal correspondence. We start with the high-confidence detections from a generic detector, then iteratively learn new video-specific features and refine the detection scores. In order to learn discriminative and compact features, we propose a new feature learning method using a deep neural network based on auto en-coders. It differs from the existing unsupervised feature learning methods in two ways: first it optimizes both discriminative and generative properties of the features simultaneously, which gives our features better discriminative ability, second, our learned features are more compact, while the unsupervised feature learning methods usually learn a redundant set of over-complete features. Extensive experimental results on person and horse detection show that significant performance improvement can be achieved with our proposed method.</p>
</div>
</li><li>
<h4>Learned versus Hand-Designed Feature Representations for 3d Agglomeration 
<a href="#Bogovic2013">[Bogovic2013]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Bogovic2013')"> [+] </a>
</h4>
<div id="abs_Bogovic2013" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >For image recognition and labeling tasks, recent results suggest that machine learning methods that rely on manually specified feature representations may be outperformed by methods that automatically derive feature representations based on the data. Yet for problems that involve analysis of 3d objects, such as mesh segmentation, shape retrieval, or neuron fragment agglomeration, there remains a strong reliance on hand-designed feature descriptors. In this paper, we evaluate a large set of hand-designed 3d feature descriptors alongside features learned from the raw data using both end-to-end and unsupervised learning techniques, in the context of agglomeration of 3d neuron fragments. By combining unsupervised learning techniques with a novel dynamic pooling scheme, we show how pure learning-based methods are for the first time competitive with hand-designed 3d shape descriptors. We investigate data augmentation strategies for dramatically increasing the size of the training set, and show how combining both learned and hand-designed features leads to the highest accuracy.</p>
</div>
</li><h3></h3>
<ul></ul>
<h3><a name="2014">2014</a></h3>
<ul><li>
<h4>OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks 
<a href="#Sermanet2014">[Sermanet2014]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Sermanet2014')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Sermanet2014" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 Framework for using CNN <ul>
<li>
 classification </li>
<li>
 localization </li>
<li>
 detection </li>
</ul>
 </li>
<li>
 Winner on localization task of ILSVRC2013 </li>
<li>
 ConvNets are trained enterily with the raw pixels </li>
<li>
 Other approaches for detection and localization </li>
<li>
 appling a sliding window over multiples scales </li>
<li>
 \dots </li>
<li>
 \dots</li>
</ul>
</div>
</li><li>
<h4>Learning Deep Face Representation 
<a href="#Fan2014">[Fan2014]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Fan2014')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Fan2014" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Face representation is a crucial step of face recognition systems. An optimal face representation should be discriminative, robust, compact, and very easy-to-implement. While numerous hand-crafted and learning-based representations have been proposed, considerable room for improvement is still present. In this paper, we present a very easy-to-implement deep learning framework for face representation. Our method bases on a new structure of deep network (called Pyramid CNN). The proposed Pyramid CNN adopts a greedy-filter-and-down-sample operation, which enables the training procedure to be very fast and computation-efficient. In addition, the structure of Pyramid CNN can naturally incorporate feature sharing across multi-scale face representations, increasing the discriminative ability of resulting representation. Our basic network is capable of achieving high recognition accuracy (85.8% on LFW benchmark) with only 8 dimension representation. When extended to feature-sharing Pyramid CNN, our system achieves the state-of-the-art performance (97.3%) on LFW benchmark. We also introduce a new benchmark of realistic face images on social network and validate our proposed representation has a good ability of generalization.</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 New deep structure Pyramid CNN </li>
<li>
 Labeled Faces in the Wild (LFW) <ul>
<li>
 $&gt; 13.000$ faces </li>
<li>
 1680 of the people have two or more distinct photos </li>
<li>
 Detected by Viola-Jones detector </li>
<li>
 http://vis-www.cs.umass.edu/lfw/ </li>
</ul>
 </li>
<li>
 State-of-the-art performance on LFW benchmark ($97.3\%$) </li>
<li>
 Good face representation <ul>
<li>
 Identity-preserving: Same person pictures close in feature space </li>
<li>
 Abstract and Compact: from high to low dimensionality </li>
<li>
 Uniform and Automatic: NO hand-crafted and hard-wired parts </li>
</ul>
 </li>
<li>
 Pyramid CNN <ul>
<li>
 ID-preserving Representation Learning: Loss functions measures distance in output feature space </li>
<li>
 Convolutions and Down-sampling </li>
<li>
 Deeper give best results, but increases rapidly the training time </li>
<li>
 Each CNN own private output layer and gets the input from the previous shared layer </li>
<li>
 Only the output of the last level network is used for the represetnation </li>
<li>
 The rest of the outputs is just for training </li>
</ul>
 </li>
<li>
 Results <ul>
<li>
 164 incorrect predictions </li>
<li>
 Some of them are incorrectly labeled </li>
<li>
 Others are very difficult for humans, because of the age or pose </li>
<li>
 On LFW benchmark achieves state-of-the-art and close to human on croped images </li>
</ul>
 </li>
<li>
 With ROC curve as a mesure there is an improvement of 0.07-0.12 with Baseline </li>
<li>
 Face recognition does not contemplate affine transformations or perspectives, </li>
<li>
 Can be difficult to apply in task such as ImageNet, where the object can be in any place and position</li>
</ul>
</div>
</li><li>
<h4>Towards Real-Time Image Understanding with Convolutional Networks 
<a href="#Farabet2014">[Farabet2014]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Farabet2014')"> [+] </a>
</h4>
<div id="abs_Farabet2014" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >One of the open questions of artificial computer vision is how to produce good internal representations of the visual world. What sort of internal representation would allow an artificial vision system to detect and classify objects into categories, independently of pose, scale, illumination, conforma- tion, and clutter? More interestingly, how could an artificial vision system learn appropriate internal representations automatically, the way animals and humans seem to learn by simply looking at the world? Another related question is that of computational tractability, and more precisely that of computational efficiency. Given a good visual represen- tation, how efficiently can it be trained, and used to encode new sensorial data. Efficiency has several dimensions: power requirements, processing speed, and memory usage. In this thesis I present three new contributions to the field of computer vision: (1) a multiscale deep convolutional network architecture to easily capture long-distance relationships between input variables in image data, (2) a tree-based algorithm to efficiently explore multiple segmentation can- didates, to produce maximally confident semantic segmentations of images, (3) a custom dataflow computer architecture optimized for the computation of convolutional networks, and similarly dense image processing models. All three contributions were produced with the common goal of getting us closer to real-time image understanding. Scene parsing consists in labeling each pixel in an image with the category of the object it belongs to. In the first part of this thesis, I propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features. Inparallel to feature extraction, a tree of segments is computed from a graph of pixel dissimilarities. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average “purity” of the class distributions, hence maximizing the overall likelihood that each segment contains a single object. The system yields record accuracies on several public benchmarks. The computation of convolutional networks, and related models heavily relies on a set of basic operators that are particularly fit for dedicated hardware implementations. In the second part of this thesis I introduce a scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms—neuFlow —and a dataflow compiler— luaFlow —that transforms high-level flow-graph representations of these al- gorithms into machine code for neuFlow. This system was designed with the goal of providing real-time detection, categorization and localization of objects in complex scenes, while consuming 10 Watts when implemented on a Xilinx Virtex 6 FPGA platform, or about ten times less than a lap- top computer, and producing speedups of up to 100 times in real-world applications (results from 2011).</p>
</div>
</li><li>
<h4>Spectral Networks and Deep Locally Connected Networks on Graphs 
<a href="#Bruna2014">[Bruna2014]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Bruna2014')"> [+] </a>
</h4>
<div id="abs_Bruna2014" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possi- ble generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low- dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.</p>
</div>
</li><li>
<h4>Large-scale Video Classification with Convolutional Neural Networks 
<a href="#Karpathy">[Karpathy]</a>
<a style="color:blue; cursor:pointer" onclick="toggle_abstract('abs_Karpathy')"> [+] </a>
<a style="color:red">my notes</a>
</h4>
<div id="abs_Karpathy" class="abstract" style="display:none"><p style="font-weight:bold">Original abstract:</p>
<p style="font-style:italic;" >Convolutional Neural Networks (CNNs) have been es-tablished as a powerful class of models for image recog-nition problems. Encouraged by these results, we pro-vide an extensive empirical evaluation of CNNs on large- scale video classification using a new dataset of 1 millionYouTube videos belonging to 487 classes. We study mul-tiple approaches for extending the connectivity of a CNNin time domain to take advantage of local spatio-temporalinformation and suggest a multiresolution, foveated archi-tecture as a promising way of speeding up the training.Our best spatio-temporal networks display significant per-formance improvements compared to strong feature-basedbaselines (55.3% to 63.9%), but only a surprisingly mod-est improvement compared to single-frame models (59.3%to 60.9%). We further study the generalization performanceof our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant per-formance improvements compared to the UCF-101 baselinemodel (63.3% up from 43.9%).</p>
<p style="font-weight:bold">My notes:</p>
<ul>
<li>
 Compare different CNN architectures for video classification </li>
<li>
 Create a new dataset with 1 million of YouTube sport videos and 487 classes </li>
<li>
 They required one month of training </li>
<li>
 Multiresolution CNNs: New CNN with low resolution context and high resolution center <ul>
<li>
 Context stream: seems to learn color filters </li>
<li>
 Fovea stream: learns grayscale features </li>
</ul>
 </li>
<li>
 Compare with and without pretraining on other dataset UCF-101 </li>
<li>
 Architectures (increasing spatio-temporal relations) <ul>
<li>
 Single frame: Classify with one single shot </li>
<li>
 Late Fusion: Classify with separate-in-time shots </li>
<li>
 Early Fusion: Classify with adjacent shots merging on first convolution layer </li>
<li>
 Slow Fusion: Classify with adjacent shots progressively mergin in upper layers </li>
</ul>
 </li>
<li>
 Results (best models): <ul>
<li>
 clip Hit, Video Hit, Video Hit top5 </li>
<li>
 42.4 60.0 78.5 Single-Frame + Multiresolution </li>
<li>
 41.9 60.9 80.2 Slow Fusion </li>
</ul>
 </li>
<li>
 Results on UCF-101 with pretraining: <ul>
<li>
 41.3 No pretraining </li>
<li>
 64.1 Fine-tune top layer </li>
<li>
 65.4 Fine-tune top 3 layers </li>
<li>
 62.2 Fine-tune all layers </li>
</ul>
 </li>
<li>
 Conclusions: <ul>
<li>
 From video classification can be derived that camera movements deteriorate the predictions </li>
<li>
 Single frame gives very good results </li>
</ul>
 </li>
<li>
 Further work: <ul>
<li>
 Apply some filter for camera movements </li>
<li>
 Explore RNN from clip-level into video-level </li>
</ul>
</li>
</ul>
</div>
</li>