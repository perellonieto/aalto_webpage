<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2008 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>Towards Real-Time Image Understanding with Convolutional Networks [11] </TITLE>
<META NAME="description" CONTENT="Towards Real-Time Image Understanding with Convolutional Networks [11] ">
<META NAME="keywords" CONTENT="summaries">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2008">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="summaries.css">

<LINK REL="next" HREF="node53.html">
<LINK REL="previous" HREF="node51.html">
<LINK REL="up" HREF="summaries.html">
<LINK REL="next" HREF="node53.html">
</HEAD>

<BODY text="#000000" bgcolor="#FFFFFF">
<!--Navigation Panel-->
<A NAME="tex2html977"
  HREF="node53.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="https://users.ics.aalto.fi/perellm1/images/icons/next.gif"></A> 
<A NAME="tex2html973"
  HREF="summaries.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="https://users.ics.aalto.fi/perellm1/images/icons/up.gif"></A> 
<A NAME="tex2html967"
  HREF="node51.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="https://users.ics.aalto.fi/perellm1/images/icons/prev.gif"></A> 
<A NAME="tex2html975"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="https://users.ics.aalto.fi/perellm1/images/icons/contents.gif"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html978"
  HREF="node53.html">Spectral Networks and Deep</A>
<B> Up:</B> <A NAME="tex2html974"
  HREF="summaries.html">Summary of References Related</A>
<B> Previous:</B> <A NAME="tex2html968"
  HREF="node51.html">Learning Deep Face Representation</A>
 &nbsp; <B>  <A NAME="tex2html976"
  HREF="node1.html">Contents</A></B> 
<BR>
<BR>
<!--End of Navigation Panel-->
<!--Table of Child-Links-->
<A NAME="CHILD_LINKS"><STRONG>Subsections</STRONG></A>

<UL>
<LI><A NAME="tex2html979"
  HREF="node52.html#SECTION000521000000000000000">Original Abstract</A>
</UL>
<!--End of Table of Child-Links-->
<HR>

<H1><A NAME="SECTION000520000000000000000">
Towards Real-Time Image Understanding with Convolutional Networks [<A
 HREF="node55.html#Farabet2014">11</A>] </A>
</H1>

<H2><A NAME="SECTION000521000000000000000">
Original Abstract</A>
</H2>

<P>
<I>
One of the open questions of artificial computer vision is how to produce good internal representations of the visual world. What sort of internal representation would allow an artificial vision system to detect and classify objects into categories, independently of pose, scale, illumination, conforma- tion, and clutter? More interestingly, how could an artificial vision system learn appropriate internal representations automatically, the way animals and humans seem to learn by simply looking at the world? Another related question is that of computational tractability, and more precisely that of computational efficiency. Given a good visual represen- tation, how efficiently can it be trained, and used to encode new sensorial data. Efficiency has several dimensions: power requirements, processing speed, and memory usage. In this thesis I present three new contributions to the field of computer vision: (1) a multiscale deep convolutional network architecture to easily capture long-distance relationships between input variables in image data, (2) a tree-based algorithm to efficiently explore multiple segmentation can- didates, to produce maximally confident semantic segmentations of images, (3) a custom dataflow computer architecture optimized for the computation of convolutional networks, and similarly dense image processing models. All three contributions were produced with the common goal of getting us closer to real-time image understanding. Scene parsing consists in labeling each pixel in an image with the category of the object it belongs to. In the first part of this thesis, I propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features. Inparallel to feature extraction, a tree of segments is computed from a graph of pixel dissimilarities. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average “purity” of the class distributions, hence maximizing the overall likelihood that each segment contains a single object. The system yields record accuracies on several public benchmarks. The computation of convolutional networks, and related models heavily relies on a set of basic operators that are particularly fit for dedicated hardware implementations. In the second part of this thesis I introduce a scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms—neuFlow —and a dataflow compiler— luaFlow —that transforms high-level flow-graph representations of these al- gorithms into machine code for neuFlow. This system was designed with the goal of providing real-time detection, categorization and localization of objects in complex scenes, while consuming 10 Watts when implemented on a Xilinx Virtex 6 FPGA platform, or about ten times less than a lap- top computer, and producing speedups of up to 100 times in real-world applications (results from 2011).
</I>
<HR>
<!--Navigation Panel-->
<A NAME="tex2html977"
  HREF="node53.html">
<IMG WIDTH="37" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next"
 SRC="https://users.ics.aalto.fi/perellm1/images/icons/next.gif"></A> 
<A NAME="tex2html973"
  HREF="summaries.html">
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up"
 SRC="https://users.ics.aalto.fi/perellm1/images/icons/up.gif"></A> 
<A NAME="tex2html967"
  HREF="node51.html">
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous"
 SRC="https://users.ics.aalto.fi/perellm1/images/icons/prev.gif"></A> 
<A NAME="tex2html975"
  HREF="node1.html">
<IMG WIDTH="65" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="contents"
 SRC="https://users.ics.aalto.fi/perellm1/images/icons/contents.gif"></A>  
<BR>
<B> Next:</B> <A NAME="tex2html978"
  HREF="node53.html">Spectral Networks and Deep</A>
<B> Up:</B> <A NAME="tex2html974"
  HREF="summaries.html">Summary of References Related</A>
<B> Previous:</B> <A NAME="tex2html968"
  HREF="node51.html">Learning Deep Face Representation</A>
 &nbsp; <B>  <A NAME="tex2html976"
  HREF="node1.html">Contents</A></B> 
<!--End of Navigation Panel-->
<ADDRESS>
Miquel Perello Nieto
2014-05-14
</ADDRESS>
</BODY>
</HTML>
